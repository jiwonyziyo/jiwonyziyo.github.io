<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>HCI on Jiwon KANG</title>
        <link>http://localhost:1313/categories/hci/</link>
        <description>Recent content in HCI on Jiwon KANG</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Sun, 19 Jan 2025 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:1313/categories/hci/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>VR Development - Roll A Ball</title>
        <link>http://localhost:1313/post/vrsetup/vrsetup/</link>
        <pubDate>Sun, 19 Jan 2025 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/post/vrsetup/vrsetup/</guid>
        <description></description>
        </item>
        <item>
        <title>Idea &amp; Prototype &amp; Evaluation</title>
        <link>http://localhost:1313/post/ideate/ideateprototypeevaluation/</link>
        <pubDate>Sun, 27 Oct 2024 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/post/ideate/ideateprototypeevaluation/</guid>
        <description>&lt;h2 id=&#34;idea&#34;&gt;Idea
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Sense Boost&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;This is an AR glasses concept designed for individuals with hearing impairments. It visualizes surrounding sounds in real time, translating sound intensity and direction into visual effects displayed on the screen. This allows users to perceive sound information through sight rather than hearing, especially for specific sounds like conversations or alerts.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;strong&gt;Focus Vision&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;These AR glasses help users concentrate on tasks by minimizing visual distractions. For example, when studying or working, the glasses detect and blur out distracting elements in the surroundings, automatically guiding the user’s attention to their main task.&lt;/li&gt;
&lt;li&gt;Useful for students, office workers, or anyone working in environments with frequent distractions, helping them to sustain their focus by reducing visual interruptions.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;&lt;strong&gt;Memory Navigator&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;This is an AR app that helps users recall experiences associated with specific places or people. For example, when visiting a particular location, users can view past photos, notes, or important events linked to that place to enhance memory recall.&lt;/li&gt;
&lt;li&gt;Suitable for people who want to remember names and faces of important individuals or recall schedules and events connected to specific locations.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;prototype&#34;&gt;Prototype
&lt;/h2&gt;&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;http://localhost:1313/images/prototye.jpg&#34; alt=&#34;prototye&#34; width=&#34;50%&#34;&gt;
&lt;/div&gt;
&lt;p&gt;With Sense Boost AR glasses, users can see sounds visually displayed in front of them. For instance, if someone on the left says, “I’m tired,” the glasses show sound waves from the left, with the spoken text “I’m tired” displayed beneath. The size of the sound wave varies depending on the volume of the sound — louder sounds appear as larger waves, while softer, distant sounds are represented by smaller, thinner waves, helping the user gauge both volume and proximity.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;evaluation&#34;&gt;Evaluation
&lt;/h2&gt;&lt;p&gt;After sharing the Sense Boost concept with friends, I realized that it was challenging for them to provide an accurate evaluation, as none of them have actual hearing difficulties. However, I recently experienced a blocked ear for about three days, which caused some discomfort. Using this experience, I tried to imagine what it might feel like not to hear fully and evaluated the concept through this perspective of limited hearing.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Increased Awareness of Surroundings&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;With a partially blocked ear, I found myself more alert to visual cues. Sense Boost&amp;rsquo;s design, which provides visual feedback for sounds, would be especially helpful in crowded or busy environments where sounds can be overwhelming or indistinguishable. I could imagine feeling reassured by visual prompts for key sounds, like a doorbell or someone calling my name.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Need for Sound Filtering Options&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Imagine when multiple sounds are displayed simultaneously, the screen can become cluttered. It was suggested to have options to filter specific types of sounds, such as displaying only conversational sounds or prioritizing important alert sounds.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Distinguishing Important Sounds&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When multiple people are speaking or when there are various sounds, it may be challenging to discern which sound is important. Adding an option to selectively emphasize crucial sounds could enhance usability.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Potential Eye Strain with Extended Use&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Since the AR display continuously presents information in front of the user’s eyes, it could lead to eye strain or discomfort over time. To address this, an alternative approach, such as a smartphone app, could reduce this strain, allowing users to view sounds only when needed.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;enhancement&#34;&gt;Enhancement
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Enhancements could include sound filtering, prioritization of important sounds, and exploring alternative devices for viewing.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A phone-based app is less intrusive than AR glasses and could be used only when needed, helping users avoid visual overload. It’s also highly portable and compatible with most devices.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>The Shape of Smartphones</title>
        <link>http://localhost:1313/post/shapeofsmart/shapeofsmartphone/</link>
        <pubDate>Sun, 20 Oct 2024 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/post/shapeofsmart/shapeofsmartphone/</guid>
        <description>&lt;img src="http://localhost:1313/images/shapeofsmart.jpg" alt="Featured image of post The Shape of Smartphones" /&gt;&lt;p&gt;The shape of smartphones is continuously evolving, driven by technological advancements and shifting consumer demands. From the earliest cell phones to today’s smartphones, designs have undergone significant changes in both form and functionality, with further innovations anticipated in the future.&lt;/p&gt;
&lt;h3 id=&#34;1-cell-phone-shapes-before-smartphones&#34;&gt;1. Cell Phone Shapes Before Smartphones
&lt;/h3&gt;&lt;p&gt;Early cell phones began as large, heavy, rectangular “bar” shapes focused primarily on basic calling functions. Later, &lt;strong&gt;flip&lt;/strong&gt; and &lt;strong&gt;slide&lt;/strong&gt; designs emerged, offering screen protection and larger keypads, enhancing ease of use.&lt;/p&gt;
&lt;h3 id=&#34;2-the-earliest-forms-of-smartphones&#34;&gt;2. The Earliest Forms of Smartphones
&lt;/h3&gt;&lt;div style=&#34;display: flex; justify-content: center; align-items: center;&#34;&gt;
  &lt;div style=&#34;margin-right: 10px; text-align: center;&#34;&gt;
    &lt;img src=&#34;http://localhost:1313/images/blackberry.jpg&#34; alt=&#34;blackberry&#34; width=&#34;60%&#34;&gt;
    &lt;p style=&#34;font-size: 12px; color: gray;&#34;&gt;Blackberry Bold 9650&lt;/p&gt;
  &lt;/div&gt;
  &lt;div style=&#34;text-align: center;&#34;&gt;
    &lt;img src=&#34;http://localhost:1313/images/iphone3.webp&#34; alt=&#34;iphone3gs&#34; width=&#34;80%&#34;&gt;
    &lt;p style=&#34;font-size: 12px; color: gray;&#34;&gt;iPhone 3gs&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;BlackBerry&lt;/strong&gt;: BlackBerry featured a compact, rectangular design with a physical keyboard and small screen, appealing especially to business users. The physical keyboard enabled fast and accurate typing, making it ideal for email and messaging.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;iPhone&lt;/strong&gt;: iPhone was revolutionary in its fully touchscreen rectangular design. By replacing the physical keyboard with a touch interface and adding a single home button, the iPhone’s minimalist design set the standard for app-centric usage, significantly impacting the shape of future smartphones.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;3-why-physical-keyboards-disappeared&#34;&gt;3. Why Physical Keyboards Disappeared
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Maximizing Screen Size&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Physical keyboards occupied valuable screen space, limiting the display size on smartphones. With the adoption of touchscreens, the entire front surface could be utilized as a display.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Enabling Diverse Input Methods&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Touchscreens support not only typing but also various gesture inputs such as multi-touch, swipe, and zooming. This allows users to interact with smartphones in a more intuitive way, going beyond simple text input.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;4-the-future-shape-of-smartphones&#34;&gt;4. The Future Shape of Smartphones
&lt;/h3&gt;&lt;p&gt;Future smartphones are expected to take on more flexible shapes, like &lt;strong&gt;foldable&lt;/strong&gt; and &lt;strong&gt;rollable&lt;/strong&gt; designs, in response to evolving user needs.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;http://localhost:1313/images/fold.jpg&#34; alt=&#34;Galaxy Fold 4&#34; width=&#34;50%&#34;&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Foldable Smartphones&lt;/strong&gt;: These devices provide a large display but fold up compactly to fit in a pocket, ideal for users who want both portability and a larger screen when needed. For instance, devices like the Galaxy Fold offer a large screen for immersive experiences but fold down for easy portability.As technology advances, we might even see multi-fold designs that can be folded multiple times, offering even larger displays while maintaining compactness for daily carry.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Rollable Smartphones&lt;/strong&gt;: Rollable smartphones, which expand when additional screen space is required, address the demand for larger screens without compromising on compactness when not in use.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion
&lt;/h3&gt;&lt;p&gt;Ultimately, smartphones have developed into more portable forms because they have become essential items we carry everywhere. At the same time, users still desire larger screens, and meeting both needs has led to concepts like foldable designs that can be carried compactly or rollable screens that expand when needed. This evolution reflects our desire for smartphones that are lighter, more compact, and easier to carry around, while still offering a larger display when required.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>VR &amp; AR &amp; MR</title>
        <link>http://localhost:1313/post/mrapp/mrapp/</link>
        <pubDate>Sun, 20 Oct 2024 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/post/mrapp/mrapp/</guid>
        <description>&lt;img src="http://localhost:1313/images/mr.jpg" alt="Featured image of post VR &amp; AR &amp; MR" /&gt;&lt;h3 id=&#34;vr-ar-and-mr&#34;&gt;VR, AR, and MR
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;VR (Virtual Reality):&lt;/strong&gt; VR immerses users in a completely virtual environment, isolating them from the real world. Users experience a 100% simulated environment. (e.g. VR museum)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;AR (Augmented Reality):&lt;/strong&gt; AR overlays digital information onto the real world. Users see information or graphics layered onto their actual surroundings. (e.g. Pokémon GO)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;MR (Mixed Reality):&lt;/strong&gt; Like AR, MR is based on the real world, but it allows digital objects to interact with physical surroundings. For example, a virtual character might sit on a real desk, blending naturally into the environment.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;future-mr-app-store-most-downloaded-apps-entertainment-focus&#34;&gt;Future MR App Store: Most Downloaded Apps (Entertainment Focus)
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Olympic Sports Experience App&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Function:&lt;/strong&gt; Allows users to experience Olympic events in MR, such as ski jumping or competitive swimming, as if they’re truly participating.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example:&lt;/strong&gt; We can perform a ski jump motion in their living room, and the MR display creates the scene of an actual jump, complete with timing and visuals.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Popularity Reason:&lt;/strong&gt; People can safely and thrillingly explore various sports without risk, appealing to adventure-seekers.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Virtual Concerts and Festivals&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Function:&lt;/strong&gt; Enables users to experience concerts or virtual festivals at home, with artists appearing as if they’re performing live in front of them.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example:&lt;/strong&gt; In a virtual concert setting, users can enjoy the music up close and interact with the crowd, feeling like they’re part of the event.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Popularity Reason:&lt;/strong&gt; For fans who can’t attend live events, this app provides a unique chance to connect closely with favorite artists.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion
&lt;/h3&gt;&lt;p&gt;One of MR’s main strengths is that it allows people to experience things that would otherwise require a lot of time and money. This accessibility increase equality, giving everyone the chance to explore, learn, and participate in experiences that were previously limited. Thanks to this, apps that make diverse experiences more accessible are likely to become highly popular in the future MR app store.&lt;/p&gt;
&lt;p&gt;Moreover, MR&amp;rsquo;s applications extend far beyond entertainment. In education, MR can immerse students in interactive learning environments, from virtual science labs to historical reenactments. In industry, MR is invaluable for training, allowing professionals to practice skills in realistic yet safe virtual settings, such as operating complex machinery or conducting medical procedures.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Kinect</title>
        <link>http://localhost:1313/post/lab4/kinect/</link>
        <pubDate>Thu, 10 Oct 2024 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/post/lab4/kinect/</guid>
        <description>&lt;img src="http://localhost:1313/images/kinectimage.png" alt="Featured image of post Kinect" /&gt;&lt;h2 id=&#34;what-is-kinect&#34;&gt;What is Kinect?
&lt;/h2&gt;&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;http://localhost:1313/images/kinect.png&#34; alt=&#34;Kinect&#34; width=&#34;50%&#34;&gt;
&lt;/div&gt;
&lt;p&gt;Kinect is a motion-sensing device used for recognizing human movement. Kinect includes a 3D depth camera, RGB camera, and microphone array, making it highly effective for tracking body positions and movements. Today, Kinect is widely used in research fields and various HCI projects beyond gaming.&lt;/p&gt;
&lt;h2 id=&#34;purpose-of-a-virtual-environment&#34;&gt;Purpose of a Virtual Environment
&lt;/h2&gt;&lt;p&gt;Using a virtual environment helps in managing project-specific dependencies independently. Each project can have its own set of libraries and versions, preventing conflicts with other projects and streamlining workflow.&lt;/p&gt;
&lt;h3 id=&#34;to-create-and-activate-a-virtual-environment&#34;&gt;To create and activate a virtual environment
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Create the virtual environment folder&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;In the project directory, such as &lt;code&gt;D:\Users\Student\Desktop\kinect&lt;/code&gt;, create a virtual environment using a hidden folder
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;python -m venv .bonjour
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;strong&gt;Activate the virtual environment&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Use this command to activate it
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;.&lt;span style=&#34;color:#ae81ff&#34;&gt;\.&lt;/span&gt;bonjour&lt;span style=&#34;color:#ae81ff&#34;&gt;\S&lt;/span&gt;cripts&lt;span style=&#34;color:#ae81ff&#34;&gt;\a&lt;/span&gt;ctivate
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;&lt;strong&gt;Check if activated&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;If activated successfully, the prompt will display &lt;code&gt;(student)&lt;/code&gt; before the directory path, indicating that the virtual environment is in use.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;kinect-and-sensor-configuration&#34;&gt;Kinect and Sensor Configuration
&lt;/h2&gt;&lt;p&gt;Kinect has multiple sensors that capture both image and depth data simultaneously. Data is stored in the project’s &lt;code&gt;data&lt;/code&gt; folder, with image and depth data organized separately.&lt;/p&gt;
&lt;h2 id=&#34;examples-of-kinect-functionalities&#34;&gt;Examples of Kinect Functionalities
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Kinect Fusion Head Scanning&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;http://localhost:1313/images/headscanning.png&#34; alt=&#34;Kinect&#34; width=&#34;50%&#34;&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;As shown in the image above, Kinect Fusion allows 3D head scanning, which reconstructs a detailed 3D model of the user&amp;rsquo;s head. Users can adjust parameters like &amp;ldquo;Volume Max Integration Weight&amp;rdquo; and &amp;ldquo;Volume Voxel Resolution&amp;rdquo; to control the detail and quality of the scan.&lt;/li&gt;
&lt;li&gt;This functionality is useful for applications that require 3D head models, which can be exported as &lt;code&gt;.STL&lt;/code&gt;, &lt;code&gt;.OBJ&lt;/code&gt;, or &lt;code&gt;.PLY&lt;/code&gt; files for use in other software.&lt;/li&gt;
&lt;/ul&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;http://localhost:1313/images/unity3dhead.png&#34; alt=&#34;Kinect&#34; width=&#34;50%&#34;&gt;
&lt;/div&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;strong&gt;Depth Sensing and Mapping&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;http://localhost:1313/images/depthsensing.png&#34; alt=&#34;Kinect&#34; width=&#34;50%&#34;&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;This image illustrates Kinect’s depth sensing capabilities, where objects are visualized based on their distance from the sensor. Black areas represent regions farthest from the Kinect, while closer areas appear in lighter shades.&lt;/li&gt;
&lt;li&gt;This depth map allows for spatial awareness, essential for applications like gesture recognition and object tracking.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;&lt;strong&gt;Face Tracking&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;http://localhost:1313/images/facetraking.png&#34; alt=&#34;Kinect&#34; width=&#34;50%&#34;&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;This image showcs Kinect’s face tracking functionality. Here, Kinect detects the user’s face and maps key facial features using a network of lines, making it ideal for applications requiring facial recognition, facial expression analysis, or real-time face-driven animations.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;real-time-data-collection-and-processing&#34;&gt;Real-Time Data Collection and Processing
&lt;/h2&gt;&lt;p&gt;To initiate real-time data collection, run the &lt;code&gt;real_time.py&lt;/code&gt; script:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;python real_time.py
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This script enables real-time detection of faces and bodies. It can be modified to add functionalities such as drawing bounding boxes around detected faces or adding other interactive elements.&lt;/p&gt;
&lt;h2 id=&#34;kinect-sdk-and-toolkit&#34;&gt;Kinect SDK and Toolkit
&lt;/h2&gt;&lt;p&gt;In our project, we’re using the Kinect SDK and the Kinect Developer Toolkit. These tools give us access to Kinect’s main features, like motion tracking, depth sensing, and 3D scanning, making it easier to work with the Kinect sensor. The SDK includes useful sample projects, such as &lt;strong&gt;Skeleton Basics&lt;/strong&gt; for tracking body movements.&lt;/p&gt;
&lt;h2 id=&#34;another-example-of-kinect---skeleton-basics&#34;&gt;Another example of Kinect - Skeleton Basics
&lt;/h2&gt;&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;video width=&#34;600&#34; controls&gt;
    &lt;source src=&#34;http://localhost:1313/videos/skeleton.mp4&#34; type=&#34;video/mp4&#34;&gt;
  &lt;/video&gt;
&lt;/div&gt;
</description>
        </item>
        <item>
        <title>HCI Researcher</title>
        <link>http://localhost:1313/post/hciresearcher/researcher/</link>
        <pubDate>Thu, 03 Oct 2024 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/post/hciresearcher/researcher/</guid>
        <description>&lt;img src="http://localhost:1313/images/researcher.png" alt="Featured image of post HCI Researcher" /&gt;&lt;h1 id=&#34;hci-researcher--young-ho-kimhttpyounghokimnet&#34;&gt;HCI Researcher : &lt;a class=&#34;link&#34; href=&#34;http://younghokim.net/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Young-Ho Kim&lt;/a&gt;
&lt;/h1&gt;&lt;h2 id=&#34;current-role-and-expertise&#34;&gt;Current Role and Expertise
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Young-Ho Kim&lt;/strong&gt; is a &lt;strong&gt;Lead Research Scientist at NAVER AI Lab&lt;/strong&gt; and a prominent researcher in the field of &lt;strong&gt;HCI&lt;/strong&gt;. His work spans across multiple domains, including &lt;strong&gt;Personal Health Informatics&lt;/strong&gt;, &lt;strong&gt;Ubiquitous Computing (UbiComp)&lt;/strong&gt;, and &lt;strong&gt;Personal Data Visualization&lt;/strong&gt;. By integrating his knowledge of &lt;strong&gt;computer science&lt;/strong&gt; and &lt;strong&gt;visual communication design&lt;/strong&gt;, Kim focuses on designing systems that enhance human-data interaction, especially in the context of &lt;strong&gt;self-tracking technologies&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;research-focus&#34;&gt;Research Focus
&lt;/h2&gt;&lt;p&gt;His &lt;strong&gt;current research&lt;/strong&gt; emphasizes the development of &lt;strong&gt;flexible self-tracking systems&lt;/strong&gt; that adapt to the needs, contexts, and preferences of individuals. His goal is to empower users to make informed decisions and behavioral changes through better data interaction. His approach often involves combining &lt;strong&gt;AI&lt;/strong&gt; and &lt;strong&gt;Natural Language Processing (NLP)&lt;/strong&gt; technologies, particularly &lt;strong&gt;Large Language Models (LLMs)&lt;/strong&gt;, to create &lt;strong&gt;intelligent self-trackers&lt;/strong&gt;. These systems help users better understand and reflect on their behaviors and health data in everyday contexts.&lt;/p&gt;
&lt;h2 id=&#34;recent-research-projects&#34;&gt;Recent Research Projects
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;CareCall&lt;/strong&gt;: A project designed to use LLMs for public health interventions. In this system, conversational AI assists socially isolated individuals by conducting regular check-up calls. The system not only gathers health metrics but also provides emotional support through empathetic conversations. The findings from this project highlight both the benefits and challenges of using LLM-driven systems in real-world public health contexts, such as balancing user expectations and handling the limitations of AI in personalization and memory.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Textoshop&lt;/strong&gt;: A novel tool that applies concepts from image editing to text manipulation. It allows users to engage in flexible and creative text editing by treating words and sentences similarly to how designers manipulate visual elements in software like Photoshop.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;notable-contributions-and-impact-on-hci&#34;&gt;Notable Contributions and Impact on HCI
&lt;/h2&gt;&lt;p&gt;Kim&amp;rsquo;s work has significantly impacted how &lt;strong&gt;AI and human-centered design&lt;/strong&gt; are integrated into real-world applications. By designing systems that bridge the gap between humans and data, he has contributed to making &lt;strong&gt;self-tracking technologies&lt;/strong&gt; more accessible and adaptable to various user needs. His contributions to &lt;strong&gt;public health technology&lt;/strong&gt;—especially in using AI to enhance social and emotional well-being—highlight the evolving role of &lt;strong&gt;HCI&lt;/strong&gt; in addressing both technical and social challenges.&lt;/p&gt;
&lt;h3 id=&#34;source&#34;&gt;Source
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Understanding the Benefits and Challenges of Deploying Conversational AI Leveraging Large Language Models for Public Health Intervention&amp;rdquo;&lt;/strong&gt; (2023)&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://dl.acm.org/doi/10.1145/3544548.3581503&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;CareCall project paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Textoshop: Interactions Inspired by Drawing Software to Facilitate Text Editing&amp;rdquo;&lt;/strong&gt; (2024)&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2409.17088&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Textoshop project paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;achievements&#34;&gt;Achievements
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Best Paper Award&lt;/strong&gt; at CHI 2023&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Best of CHI Honorable Mention&lt;/strong&gt; in 2021&lt;/li&gt;
&lt;li&gt;Recipient of the &lt;strong&gt;International Postdoc Fellowship&lt;/strong&gt; from the National Research Foundation of Korea (2019)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion
&lt;/h2&gt;&lt;p&gt;Young-Ho Kim&amp;rsquo;s work at the forefront of &lt;strong&gt;HCI research&lt;/strong&gt; continues to shape how we think about user interaction with data and technology. His contributions to &lt;strong&gt;flexible self-tracking systems&lt;/strong&gt; and &lt;strong&gt;AI-driven health interventions&lt;/strong&gt; exemplify the power of integrating &lt;strong&gt;advanced AI&lt;/strong&gt; with &lt;strong&gt;human-centered design&lt;/strong&gt;, making significant strides in both public health and everyday technology interactions.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Input Devices and Interaction Paradigm</title>
        <link>http://localhost:1313/post/devices/inputdevices/</link>
        <pubDate>Thu, 03 Oct 2024 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/post/devices/inputdevices/</guid>
        <description>&lt;img src="http://localhost:1313/images/leap.jpg" alt="Featured image of post Input Devices and Interaction Paradigm" /&gt;&lt;h1 id=&#34;the-leap-motion-controller&#34;&gt;The Leap Motion Controller
&lt;/h1&gt;&lt;p&gt;The &lt;strong&gt;Leap Motion Controller&lt;/strong&gt; (introduced in 2013) is a &lt;strong&gt;gesture-based input device&lt;/strong&gt; designed to allow users to control on-screen elements through &lt;strong&gt;hand and finger movements&lt;/strong&gt;. Unlike traditional input devices like the mouse and keyboard, Leap Motion provided a &lt;strong&gt;touch-free, 3D interaction&lt;/strong&gt; experience. It could track users&amp;rsquo; hand movements in real time, within an interaction space of about 8 cubic feet.&lt;/p&gt;
&lt;h3 id=&#34;gesture-based-user-interface&#34;&gt;Gesture-Based User Interface
&lt;/h3&gt;&lt;p&gt;Leap Motion falls under &lt;strong&gt;Gesture-Based User Interfaces (GBUI)&lt;/strong&gt;, which allow interaction through body movements. Specifically, it’s part of the broader &lt;strong&gt;Natural User Interface (NUI)&lt;/strong&gt; category, focusing on natural and intuitive interactions.&lt;/p&gt;
&lt;h2 id=&#34;why-leap-motion-failed&#34;&gt;Why Leap Motion Failed
&lt;/h2&gt;&lt;p&gt;Despite its innovative concept, Leap Motion did not gain widespread adoption due to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Limited Use Cases&lt;/strong&gt;: Its primary applications, like &lt;strong&gt;gaming&lt;/strong&gt; and &lt;strong&gt;3D modeling&lt;/strong&gt;, appealed only to niche audiences. Most users found traditional input methods more practical for everyday tasks.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Tracking Issues&lt;/strong&gt;: Many users reported inconsistent accuracy and latency, particularly when hands moved quickly or left the interaction zone. This made it hard to rely on for precise tasks, such as &lt;strong&gt;3D design&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Lack of Developer Support&lt;/strong&gt;: Limited developer interest meant fewer applications that leveraged Leap Motion’s capabilities, reducing user incentive to adopt the device.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Competition with VR/AR&lt;/strong&gt;: As VR and AR technology gained popularity, Leap Motion struggled to stay relevant despite attempts to integrate with VR headsets.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;potential-for-future-success&#34;&gt;Potential for Future Success
&lt;/h2&gt;&lt;p&gt;While Leap Motion didn’t succeed initially, the technology might find new applications as &lt;strong&gt;gesture-based interactions&lt;/strong&gt; become more important in &lt;strong&gt;VR and AR&lt;/strong&gt; environments. With its 2019 acquisition by &lt;strong&gt;Ultraleap&lt;/strong&gt;, which combines gesture control with haptic feedback, this technology could offer more immersive experiences in fields like &lt;strong&gt;healthcare&lt;/strong&gt; and &lt;strong&gt;remote collaboration&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion
&lt;/h3&gt;&lt;p&gt;The Leap Motion Controller was a forward-thinking product that didn’t find immediate success due to limited use cases, technical challenges, and competition. However, with advances in &lt;strong&gt;machine learning&lt;/strong&gt; and &lt;strong&gt;computer vision&lt;/strong&gt;, its underlying technology may still shape future human-computer interactions.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>My First Unity Project - Roll a ball</title>
        <link>http://localhost:1313/post/lab3/unity3dapplication/</link>
        <pubDate>Thu, 03 Oct 2024 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/post/lab3/unity3dapplication/</guid>
        <description>&lt;img src="http://localhost:1313/images/rollaballimage.png" alt="Featured image of post My First Unity Project - Roll a ball" /&gt;&lt;p&gt;Today, I’ll introduce my first Unity 3D project, the classic Roll-a-Ball game.&lt;/p&gt;
&lt;h4 id=&#34;unity-setup&#34;&gt;Unity Setup
&lt;/h4&gt;&lt;p&gt;If you’re interested in how to set up Unity, check out my &lt;a class=&#34;link&#34; href=&#34;https://jiwonyziyo.github.io/post/lab2/setupunity/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Set up Unity&lt;/a&gt;. I cover everything from downloading Unity Hub to configuring the environment for this project.&lt;/p&gt;
&lt;h4 id=&#34;building-the-basics&#34;&gt;Building the Basics
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Setting the Scene:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I created a new 3D project in Unity and got started by adding a flat &lt;strong&gt;Plane&lt;/strong&gt; as the ground.&lt;/li&gt;
&lt;li&gt;Resized it to &lt;code&gt;(2, 1, 2)&lt;/code&gt; to give our rolling ball plenty of space to move.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Adding the Player:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Next, I created a &lt;strong&gt;Sphere&lt;/strong&gt; object and named it &lt;strong&gt;Player&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Moved the sphere up slightly by setting the Y coordinate to &lt;code&gt;0.5&lt;/code&gt;. This gave it a realistic &amp;ldquo;standing&amp;rdquo; position on the ground.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Adding the Rigidbody Component:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To enable physics interactions, I added a &lt;strong&gt;Rigidbody&lt;/strong&gt; component to the Player object. This step allows the ball to respond to forces and gravity.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Creating a Player Controller Script:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I added a script called &lt;strong&gt;PlayerController.cs&lt;/strong&gt; to manage the ball&amp;rsquo;s movement.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-csharp&#34; data-lang=&#34;csharp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;using&lt;/span&gt; UnityEngine;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;public&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;PlayerController&lt;/span&gt; : MonoBehaviour
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;{
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;public&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; speed = &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;private&lt;/span&gt; Rigidbody rb;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; Start()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        rb = GetComponent&amp;lt;Rigidbody&amp;gt;();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; FixedUpdate()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; moveHorizontal = Input.GetAxis(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Horizontal&amp;#34;&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; moveVertical = Input.GetAxis(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Vertical&amp;#34;&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        Vector3 movement = &lt;span style=&#34;color:#66d9ef&#34;&gt;new&lt;/span&gt; Vector3(moveHorizontal, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.0f&lt;/span&gt;, moveVertical);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        rb.AddForce(movement * speed);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This script got the ball rolling!&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;setting-up-the-camera&#34;&gt;Setting Up the Camera
&lt;/h4&gt;&lt;p&gt;For a more immersive experience, I wanted the camera to follow the player. Initially, I tried making the &lt;strong&gt;Main Camera&lt;/strong&gt; a child of the Player object, but that led to a weird, shaky view. So instead, I wrote a &lt;strong&gt;CameraController&lt;/strong&gt; script that allowed the camera to follow the ball smoothly, using a fixed offset.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-csharp&#34; data-lang=&#34;csharp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;public&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;CameraController&lt;/span&gt; : MonoBehaviour
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;{
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;public&lt;/span&gt; GameObject player;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;private&lt;/span&gt; Vector3 offset;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; Start()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        offset = transform.position - player.transform.position;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; LateUpdate()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        transform.position = player.transform.position + offset;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;adding-the-pick-ups-with-score-variations&#34;&gt;Adding the Pick-Ups with Score Variations
&lt;/h4&gt;&lt;p&gt;I added two types of collectible cubes: &lt;code&gt;PickUpGreenParents&lt;/code&gt; and &lt;code&gt;PickUpOrangeParents&lt;/code&gt;. Each of them affects the score differently:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Green Pick-Ups&lt;/strong&gt;: When the player collects these, the score increases by &lt;strong&gt;+1&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Orange Pick-Ups&lt;/strong&gt;: Collecting an orange pick-up decreases the score by &lt;strong&gt;-1&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Creating Prefabs for Green and Orange Pick-Ups&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I created one green and one orange cube, then saved them as separate prefabs: &lt;code&gt;PickUpGreenParents&lt;/code&gt; (colored green) and &lt;code&gt;PickUpOrangeParents&lt;/code&gt; (colored orange). Prefabs made it easy to duplicate and spread these objects around the scene while keeping the setup consistent.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Configuring Collider and Trigger&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Each prefab was tagged with &lt;code&gt;PickUpGreenParents&lt;/code&gt; or &lt;code&gt;PickUpOrangeParents&lt;/code&gt; so I could handle them differently in the script. I also added a &lt;strong&gt;Collider&lt;/strong&gt; component to each with &amp;ldquo;Is Trigger&amp;rdquo; enabled.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Updating the PlayerController Script&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To manage scoring, I modified the &lt;strong&gt;PlayerController&lt;/strong&gt; script to check each collected object’s tag. Based on whether the tag was &lt;code&gt;PickUpGreenParents&lt;/code&gt; or &lt;code&gt;PickUpOrangeParents&lt;/code&gt;, the script either increased or decreased the score.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-csharp&#34; data-lang=&#34;csharp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;private&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; OnTriggerEnter(Collider other)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;{
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (other.gameObject.CompareTag(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;PickUpGreenParents&amp;#34;&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        other.gameObject.SetActive(&lt;span style=&#34;color:#66d9ef&#34;&gt;false&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        count = count + &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;;  &lt;span style=&#34;color:#75715e&#34;&gt;// Increases score by 1 for green&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        SetCountText();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (other.gameObject.CompareTag(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;PickUpOrangeParents&amp;#34;&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        other.gameObject.SetActive(&lt;span style=&#34;color:#66d9ef&#34;&gt;false&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        count = count - &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;;  &lt;span style=&#34;color:#75715e&#34;&gt;// Decreases score by 1 for orange&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        SetCountText();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;&lt;strong&gt;Using the SetCountText Method&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;I used the &lt;code&gt;SetCountText()&lt;/code&gt; method to update the score display each time a pick-up was collected, so the player sees their updated score in real-time.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-csharp&#34; data-lang=&#34;csharp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; SetCountText()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        countText.text = &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Count: &amp;#34;&lt;/span&gt; + count.ToString();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (count &amp;gt;= &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;) 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            winTextObject.SetActive(&lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h4 id=&#34;the-ui-text-hurdle-adding-the-score-and-win-text&#34;&gt;The UI Text Hurdle: Adding the Score and Win Text
&lt;/h4&gt;&lt;p&gt;Here&amp;rsquo;s where I had a small error. I wanted to display the score and a &amp;ldquo;You Win!&amp;rdquo; message, but Unity wasn’t showing the UI objects. I realized I had forgotten to make the &lt;strong&gt;CountText&lt;/strong&gt; and &lt;strong&gt;WinTextObject&lt;/strong&gt; fields in my script public, which prevented me from linking them in the Inspector. Without these fields being public, Unity can’t display the UI elements on screen, so I was left staring at a blank display.&lt;/p&gt;
&lt;p&gt;After making them public, I linked the &lt;strong&gt;CountText&lt;/strong&gt; and &lt;strong&gt;WinTextObject&lt;/strong&gt; objects in the Inspector. Here is the updat code !&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-csharp&#34; data-lang=&#34;csharp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;public&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;PlayerController&lt;/span&gt; : MonoBehaviour
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;{
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;public&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; speed = &lt;span style=&#34;color:#ae81ff&#34;&gt;10f&lt;/span&gt;; 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;public&lt;/span&gt; TextMeshProUGUI countText;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;public&lt;/span&gt; GameObject winTextObject;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;private&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; count;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;http://localhost:1313/images/rollaball.png&#34; alt=&#34;Kinect&#34; width=&#34;100%&#34;&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h4 id=&#34;adding-accelerometer-input-for-mobile&#34;&gt;Adding Accelerometer Input for Mobile
&lt;/h4&gt;&lt;p&gt;I implemented &lt;strong&gt;accelerometer-based movement&lt;/strong&gt;. This allowed the player to control the ball by tilting their device—a great way to enhance gameplay on smartphones!&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Enabling the Accelerometer&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To use the accelerometer, I first checked if the device is a mobile device by using an &lt;code&gt;isMobileBuild&lt;/code&gt; flag. I also used &lt;code&gt;InputSystem.EnableDevice&lt;/code&gt; to activate the accelerometer input when running on mobile.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Updating Movement Logic&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In the &lt;code&gt;FixedUpdate&lt;/code&gt; method, I checked for accelerometer data. If it was available, I used that data for movement; otherwise, I defaulted to keyboard input (for desktop testing). Here’s the updated &lt;code&gt;FixedUpdate&lt;/code&gt; code:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-csharp&#34; data-lang=&#34;csharp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;private&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; FixedUpdate()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;{
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    Vector3 movement = Vector3.zero;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (isMobileBuild &amp;amp;&amp;amp; UnityEngine.InputSystem.Accelerometer.current != &lt;span style=&#34;color:#66d9ef&#34;&gt;null&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// Get accelerometer data for movement&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        Vector3 acceleration = UnityEngine.InputSystem.Accelerometer.current.acceleration.ReadValue();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        AccText.text = &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Accelerometer: &amp;#34;&lt;/span&gt; + acceleration.ToString(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;F6&amp;#34;&lt;/span&gt;);  &lt;span style=&#34;color:#75715e&#34;&gt;// Display accelerometer data&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// Map accelerometer data to movement (adjust axes if needed based on phone orientation)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        movement = &lt;span style=&#34;color:#66d9ef&#34;&gt;new&lt;/span&gt; Vector3(acceleration.x, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.0f&lt;/span&gt;, acceleration.y);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// Use input from the non-mobile build (keyboard/controller input)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        movement = &lt;span style=&#34;color:#66d9ef&#34;&gt;new&lt;/span&gt; Vector3(movementX, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.0f&lt;/span&gt;, movementY);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// Apply movement force to the rigidbody&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    rb.AddForce(movement * speed);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// Enable accelerometer if running on a mobile device&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (isMobileBuild &amp;amp;&amp;amp; UnityEngine.InputSystem.Accelerometer.current != &lt;span style=&#34;color:#66d9ef&#34;&gt;null&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;{
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    InputSystem.EnableDevice(UnityEngine.InputSystem.Accelerometer.current);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Displaying Accelerometer Data&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To help debug, I displayed the accelerometer data on the screen using a &lt;code&gt;Text&lt;/code&gt; component (&lt;code&gt;AccText&lt;/code&gt;). This allowed me to confirm the data being received and adjust the movement accordingly.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h4 id=&#34;deploying-as-a-mobile-application&#34;&gt;Deploying as a Mobile Application
&lt;/h4&gt;&lt;p&gt;After implementing accelerometer controls, I wanted to test the game on a mobile device. Since I’m using a &lt;strong&gt;MacBook and iPhone&lt;/strong&gt;, I needed &lt;strong&gt;Xcode&lt;/strong&gt; to build and deploy the app.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Xcode Requirement for iOS Testing&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Initially, I planned to use &lt;strong&gt;Unity Remote 5&lt;/strong&gt; for quick mobile testing. I went to &lt;strong&gt;File &amp;gt; Build Settings &amp;gt; iOS &amp;gt; Switch Platform&lt;/strong&gt; to change the platform to iOS. I also opened &lt;strong&gt;Player Settings&lt;/strong&gt; to configure my project for iOS. However, I discovered that without Xcode installed, Unity wouldn’t let me select &lt;strong&gt;iPhone&lt;/strong&gt; under &lt;strong&gt;Editor &amp;gt; Device&lt;/strong&gt; in &lt;strong&gt;Project Settings&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Installing Xcode&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To resolve this, I downloaded and installed Xcode from the App Store. This process took a while, but it’s essential for iOS development on Unity.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Using Unity Remote 5 for Quick Testing&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;With Xcode installed, I could finally set the platform to iOS and select &lt;strong&gt;iPhone&lt;/strong&gt; under &lt;strong&gt;Editor &amp;gt; Device&lt;/strong&gt; in &lt;strong&gt;Project Settings&lt;/strong&gt;. I downloaded &lt;strong&gt;Unity Remote 5&lt;/strong&gt; on my iPhone, connected it, and could now use the remote app to test things like accelerometer movement without needing to build and deploy the app.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;demo&#34;&gt;Demo
&lt;/h2&gt;&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;video width=&#34;600&#34; controls&gt;
    &lt;source src=&#34;http://localhost:1313/videos/rollaball.mov&#34; type=&#34;video/mp4&#34;&gt;
  &lt;/video&gt;
&lt;/div&gt;
</description>
        </item>
        <item>
        <title>Ultimate Display by Ivan Sutherland </title>
        <link>http://localhost:1313/post/readivan/ultimatedisplay/</link>
        <pubDate>Thu, 03 Oct 2024 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/post/readivan/ultimatedisplay/</guid>
        <description>&lt;img src="http://localhost:1313/images/ivan.jpg" alt="Featured image of post Ultimate Display by Ivan Sutherland " /&gt;&lt;h1 id=&#34;reflections-on-ivan-sutherlands-the-ultimate-display&#34;&gt;Reflections on Ivan Sutherland’s “The Ultimate Display”
&lt;/h1&gt;&lt;p&gt;In 1965, &lt;strong&gt;Ivan Sutherland&lt;/strong&gt; wrote &lt;em&gt;“The Ultimate Display”&lt;/em&gt;, imagining a future where computers could create realistic, immersive worlds. His ideas basically set the stage for &lt;strong&gt;Virtual Reality (VR)&lt;/strong&gt; and &lt;strong&gt;Augmented Reality (AR)&lt;/strong&gt; as we know them today.&lt;/p&gt;
&lt;h2 id=&#34;what-sutherland-predicted-and-whats-happening-now&#34;&gt;What Sutherland Predicted and What’s Happening Now
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;VR and AR&lt;/strong&gt;: Sutherland envisioned virtual worlds that felt as real as the physical world. Today, VR headsets like &lt;strong&gt;Oculus Rift&lt;/strong&gt; and AR tools like &lt;strong&gt;Microsoft HoloLens&lt;/strong&gt; make that idea real by letting us experience and interact with digital content in immersive ways.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Touch and Feel in VR&lt;/strong&gt;: Sutherland also imagined being able to feel virtual objects, like sitting on a virtual chair that “feels” real. With &lt;strong&gt;haptic technology&lt;/strong&gt; (gloves, vests, etc.), we can now feel textures, pressure, and vibrations in VR, bringing us closer to that vision.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Interactive Graphics&lt;/strong&gt;: He foresaw real-time interactive graphics, where users could control and manipulate digital objects. This idea led to the development of game engines like &lt;strong&gt;Unreal Engine&lt;/strong&gt; and &lt;strong&gt;Unity&lt;/strong&gt;, which power modern video games, training simulations, and 3D modeling.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;what-could-still-become-reality&#34;&gt;What Could Still Become Reality
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Full Sensory VR&lt;/strong&gt;: Sutherland hinted at VR that stimulates all senses, including taste and smell. While current VR focuses on sight and sound, researchers are experimenting with &lt;strong&gt;smell and taste simulation&lt;/strong&gt;. One day, VR could fully engage all our senses.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Brain-Computer Interfaces (BCI)&lt;/strong&gt;: Although Sutherland didn’t directly mention it, his vision of seamless interaction hints at controlling VR with our minds. Projects like &lt;strong&gt;Neuralink&lt;/strong&gt; are working on this, which could let us move in VR worlds using just our thoughts.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Shared Virtual Worlds (Metaverse)&lt;/strong&gt;: Sutherland imagined shared virtual experiences. Today, companies like &lt;strong&gt;Meta&lt;/strong&gt; are building the &lt;strong&gt;metaverse&lt;/strong&gt;—a persistent virtual world where people can meet, work, and socialize.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion
&lt;/h2&gt;&lt;p&gt;Ivan Sutherland’s ideas in &lt;em&gt;“The Ultimate Display”&lt;/em&gt; were incredibly ahead of his time. Many of his predictions are now reality, and his vision still inspires new advancements in VR, AR, and immersive tech. His work is a reminder of where we started and where the future of human-computer interaction might take us.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Source&lt;/strong&gt;: &lt;a class=&#34;link&#34; href=&#34;http://worrydream.com/refs/Sutherland%20-%20The%20Ultimate%20Display.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;The Ultimate Display by Ivan Sutherland&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Affordances</title>
        <link>http://localhost:1313/post/affordances/affordances/</link>
        <pubDate>Thu, 19 Sep 2024 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/post/affordances/affordances/</guid>
        <description>&lt;img src="http://localhost:1313/images/affordance.jpg" alt="Featured image of post Affordances" /&gt;&lt;p style=&#34;font-size: 12px; color: gray;&#34;&gt;Image Source: &lt;a href=https://medium.com/@akadiyala/role-of-affordances-in-digital-transformation-and-internet-of-things-fa2896970480 style=&#34;color: gray;&#34;&gt; Anant Kadiyala &lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Affordance gives users a visual hint on what actions they can take.&lt;/p&gt;
&lt;p&gt;For example, when we see a button, we instinctively want to press it, or when we see a switch, we feel like pulling it. On an app or website, a rectangular box with a border makes us think we can click and type into it. These cues play into human psychology.&lt;/p&gt;
&lt;p&gt;Thus, affordance is a crucial element of visual user interfaces. The clearer the visual cues are, the less ambiguity there is for the user to understand what action is expected.&lt;/p&gt;
&lt;p&gt;In mobile environments, the importance of affordance becomes even more pronounced, as the small screen size and limited space make it harder for affordance to be as visually obvious. Therefore, clear affordance is critical to ensure smooth user interaction on mobile devices.&lt;/p&gt;
&lt;h2 id=&#34;good-case---traffic-lights-that-show-the-remaining-time&#34;&gt;Good case - Traffic lights that show the remaining time
&lt;/h2&gt;&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;http://localhost:1313/images/trafficlight.jpg&#34; alt=&#34;Traffic light&#34; width=&#34;50%&#34;&gt;
  &lt;p style=&#34;font-size: 12px; color: gray;&#34;&gt;Image Source: &lt;a href=&#34;https://busan.fnnews.com/news/202004071520322481&#34; style=&#34;color: gray;&#34;&gt;Busan News&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;1. Clear Information Display&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Traffic lights that show the remaining time provide users with clear information on how much time is left before the signal changes. Pedestrians can easily determine whether they have enough time to cross the street, and drivers can predict when the signal will switch. This intuitive display helps users make more informed and safer decisions.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;2. Behavior Guidance&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The countdown timer allows both pedestrians and drivers to act accordingly. Pedestrians can choose to cross quickly if there is enough time, or wait for the next signal if time is running out. This guides user behavior and enhances traffic safety by reducing risky actions.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;3. Meeting User Expectations&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In addition to simply showing red or green lights, the countdown timer allows users to predict signal changes more accurately. This design meets user expectations by providing precise information, reducing stress at intersections, and improving overall traffic flow.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;bad-case---keyboard&#34;&gt;Bad case - Keyboard
&lt;/h2&gt;&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;http://localhost:1313/images/keyboard.jpg&#34; alt=&#34;Keyboard&#34; width=&#34;50%&#34;&gt;
  &lt;p style=&#34;font-size: 12px; color: gray;&#34;&gt;Image Source: https://www.clien.net/service/board/park/16744923&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The placement of a power button above the delete key on a keyboard is a bad example of affordance.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. Risk of Accidental Use&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When the power button is located too close to the delete key, users are more likely to press it accidentally, potentially shutting down the system unintentionally. The functions are too distinct to be placed so closely.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;2. Functional Mismatch&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The delete key is frequently used, whereas the power button is not. Placing such an important function near a less frequently used key increases the risk of errors and reduces the overall usability.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;3. Contrary to User Expectations&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Users do not expect two very different functions to be positioned so closely together. This placement disrupts the intuitive understanding of how the keyboard should work.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;solutions&#34;&gt;&lt;em&gt;Solutions&lt;/em&gt;
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;1. Relocate the Power Button&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The most straightforward solution is to move the power button to a location further away from frequently used keys like the delete key. For example, the power button could be placed at the right-top corner of the keyboard, near the function keys or in a separate, more isolated area where it’s less likely to be pressed by accident.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;2. Add a Confirmation Step&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Implementing a confirmation step when the power button is pressed would prevent accidental shutdowns. For instance, instead of instantly shutting down the system, the button could trigger a prompt asking the user to confirm the action.&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Dark Design Patterns</title>
        <link>http://localhost:1313/post/dark-design-patterns/darkdesignpattern/</link>
        <pubDate>Thu, 19 Sep 2024 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/post/dark-design-patterns/darkdesignpattern/</guid>
        <description>&lt;img src="http://localhost:1313/images/darkpatterndesign.jpg" alt="Featured image of post Dark Design Patterns" /&gt;&lt;p style=&#34;font-size: 12px; color: gray;&#34;&gt;Image Source: &lt;a href=&#34;https://polytechnic.purdue.edu/newsroom/dark-patterns-user-experience-design-manipulates-consumers&#34; style=&#34;color: gray;&#34;&gt;By John O&#39;Malley &lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dark patterns&lt;/strong&gt; are UI/UX design techniques intentionally crafted to exploit human psychology and trick users into doing things they don’t necessarily want to do. The British UX designer &lt;strong&gt;Harry Brignull&lt;/strong&gt;, who first coined the term, described dark patterns as follows&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;“A carefully crafted user interface designed to trick users into performing actions such as signing up for insurance or signing up for recurring bills.”&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In other words, dark patterns refer to UI/UX that is cleverly and intentionally designed to elicit certain actions from users, whether they want to or not. These designs often serve business goals, such as increasing subscriptions or data collection, but they do so at the expense of the user’s autonomy, leading to unintended actions like accidental purchases or subscriptions.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;types-of-dark-patterns-design&#34;&gt;Types of Dark Patterns Design
&lt;/h3&gt;&lt;p&gt;Here are five types of dark patterns commonly seen&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Nagging&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;http://localhost:1313/images/nagging.jpg&#34; alt=&#34;Nagging dark pattern example&#34; width=&#34;30%&#34;&gt;
  &lt;p style=&#34;font-size: 12px; color: gray;&#34;&gt;Image Source: &lt;a href=&#34;https://www.researchgate.net/figure/Example-of-nagging-behavior-on-Instagram-where-a-modal-dialogue-provides-no-opportunity_fig1_322916969&#34; style=&#34;color: gray;&#34;&gt;ResearchGate&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Nagging&lt;/strong&gt; occurs when an app or website repeatedly interrupts the user’s progress with prompts or messages, draining their time and attention. These interruptions can make the user feel pressured or annoyed, leading them to eventually agree to the message or request—even if it’s not what they want—just to move forward.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If the interruptions happen frequently, the user might decide that giving in to the prompt is easier than continuing to dismiss it. This pattern is commonly seen in requests to subscribe to premium services, allow notifications, or share personal data, and it can result in a frustrating user experience.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;strong&gt;Obstruction&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;http://localhost:1313/images/obstruction.jpg&#34; alt=&#34;Obstruction dark pattern example&#34; width=&#34;50%&#34;&gt;
  &lt;p style=&#34;font-size: 12px; color: gray;&#34;&gt;Image Source: &lt;a href=&#34;https://www.deceptive.design/types/obstruction&#34; style=&#34;color: gray;&#34;&gt;Norwegian Consumer Council, 2018&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Obstruction&lt;/strong&gt; involves intentionally making certain tasks difficult or confusing for the user. It artificially complicates the steps required to perform actions that the user might want to avoid, such as canceling a service, deleting an account, or disabling ads. Designers create complex menu structures, lengthy procedures, and multiple confirmation steps to increase the likelihood that the user will give up.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For example, in this image, Facebook requires users to review and adjust their data sharing settings across multiple screens. Users must navigate through detailed information and various options to disable data sharing with advertisers. By making this process lengthy and complex, users are more likely to accept the default settings, even if they intended to limit data sharing.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;&lt;strong&gt;Sneaking&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;http://localhost:1313/images/sneaking.jpg&#34; alt=&#34;Sneaking dark pattern example&#34; width=&#34;50%&#34;&gt;
  &lt;p style=&#34;font-size: 12px; color: gray;&#34;&gt;Image Source: &lt;a href=&#34;https://app.uxcel.com/lessons/dark-patterns-024&#34; style=&#34;color: gray;&#34;&gt;Uxcel&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Sneaking&lt;/strong&gt; occurs when important information, such as additional fees or terms, is hidden from the user until the last possible moment. This pattern is often used to make a product or service appear cheaper or more attractive than it actually is, only revealing the true cost or consequences just before the user commits.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For example, an accomodation booking site may show a price at a discounted price, but hidden fees like cleaning costs is only added at the checkout stage. This tactic leaves the user feeling deceived and frustrated because they weren’t provided with full transparency from the beginning.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;&lt;strong&gt;Interface Interference&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;http://localhost:1313/images/interfaceinterference.jpg&#34; alt=&#34;Interface Interference example&#34; width=&#34;50%&#34;&gt;
  &lt;p style=&#34;font-size: 12px; color: gray;&#34;&gt;Image Source: &lt;a href=&#34;https://www.emailtooltester.com/en/blog/dark-patterns-canceling-subscription-report/&#34; style=&#34;color: gray;&#34;&gt;By Cai &amp; Roberta&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Interface interference&lt;/strong&gt; manipulates the design of user interface elements—such as buttons, links, or menus—to confuse or mislead the user. This often involves making certain options (like opting out or declining a service) difficult to see or access, or placing desired actions in locations where users are less likely to find them.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In this example, the user is trying to cancel a subscription, but the interface prominently displays a blue button that offers a discount (“Give me US$1.00 off”), making it more noticeable than the “Continue to cancel” option. This visual manipulation influences the user’s decision, encouraging them to take the action that benefits the company (keeping the subscription) rather than proceeding with the cancellation.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;&lt;strong&gt;Forced Action&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;http://localhost:1313/images/forceaction.png&#34; alt=&#34;Forced action example&#34; width=&#34;50%&#34;&gt;
  &lt;p style=&#34;font-size: 12px; color: gray;&#34;&gt;Image Source: &lt;a href=&#34;https://darkpatterns.uxp2.com/pattern/windows-10-forced-update/&#34; style=&#34;color: gray;&#34;&gt;blog&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Forced Action&lt;/strong&gt; refers to situations where the user is required to perform a specific action in order to continue using a service or complete a task. This pattern typically forces users to agree to terms, share personal information, or sign up for a service they may not want, simply to proceed with what they were doing. It can make users feel that they have no other option but to comply.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For instance, in this image, a user simply wants to shut down or restart their computer, but the only options available are “Update and shut down” or “Update and restart.” This design forces the user to perform an update, even if they would prefer to delay it, limiting their choice and making them feel compelled to take an action they may not want at that moment.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;examples&#34;&gt;Examples
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Instasize Premium Subscription&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Instasize is an app that helps people resize photos for Instagram, ensuring that they don’t get cropped when posted.&lt;/li&gt;
&lt;/ul&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;http://localhost:1313/images/instasize.png&#34; alt=&#34;Forced action example&#34; width=&#34;30%&#34;&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Dark Pattern Explanation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Nagging&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;While I’m editing photos, a premium subscription pop-up frequently appears, interrupting my workflow. Even though I’m not interested in upgrading, I keep seeing the same subscription prompt repeatedly, which creates a sense of fatigue and frustration due to the constant interruptions.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Forced Action&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The pop-up doesn’t have a close button, and I have to wait around 10 seconds for it to disappear before I can continue using the app. This lack of a “close” option makes me feel pressured into considering the premium subscription since I’m essentially held back from my task.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;How to Redesign It&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Add an “X” or “Close” button to the pop-up so that users like me can dismiss it immediately if we’re not interested, giving us full control over our experience.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Include an option in the app settings that lets users turn off subscription prompts if they do not wish to see them.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;strong&gt;Skyscanner Price Discrepancy by Country Setting&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Skyscanner is a flight search engine that allows users to compare flight prices across different airlines and book their travel at the best price. I often use Skyscanner to find affordable flights for my travels.&lt;/li&gt;
&lt;/ul&gt;
&lt;div style=&#34;display: flex; justify-content: center; align-items: center;&#34;&gt;
  &lt;div style=&#34;margin-right: 10px; text-align: center;&#34;&gt;
    &lt;img src=&#34;http://localhost:1313/images/skyscannerko.png&#34; alt=&#34;Price in Korea Setting&#34; width=&#34;100%&#34;&gt;
    &lt;p style=&#34;font-size: 12px; color: gray;&#34;&gt;Price in Korea Setting&lt;/p&gt;
  &lt;/div&gt;
  &lt;div style=&#34;text-align: center;&#34;&gt;
    &lt;img src=&#34;http://localhost:1313/images/skyscannerfr.png&#34; alt=&#34;Price in France Setting&#34; width=&#34;100%&#34;&gt;
    &lt;p style=&#34;font-size: 12px; color: gray;&#34;&gt;Price in France Setting&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Dark Pattern Explanation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sneaking&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When I set my country to Korea and search for a flight from Paris to Korea, the price displayed differs from the price shown when I set the country to France. This pricing difference is not clearly communicated to users, and the cheaper option is hidden unless they change their location setting. This can cause users to unknowingly pay more for the same ticket.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Interface Interference&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The interface automatically adjusts the price based on the selected country, limiting my ability to see or choose the best price across locations. Because there is no transparent way to compare prices across countries, users like me may miss out on the lower price option.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;How to Redesign It&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Allow users to easily see the price differences between countries by adding a &amp;ldquo;Compare prices across countries&amp;rdquo; option. This would help users like me make informed choices by seeing all available prices.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Display a note explaining that prices may vary depending on the country setting, giving users full transparency about potential savings if they adjust their location.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Gestalt Law</title>
        <link>http://localhost:1313/post/gestalt-laws/gestaltlaw/</link>
        <pubDate>Thu, 19 Sep 2024 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/post/gestalt-laws/gestaltlaw/</guid>
        <description>&lt;img src="http://localhost:1313/images/gestalt.jpg" alt="Featured image of post Gestalt Law" /&gt;&lt;p style=&#34;font-size: 12px; color: gray;&#34;&gt;Image Source: &lt;a href=&#34;https://medium.com/ringcentral-ux/gestalt-principles-learn-how-to-influence-perception-83112932d0bc&#34; style=&#34;color: gray;&#34;&gt;Gestalt Principles&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Gestalt&lt;/strong&gt; is a German word meaning &amp;lsquo;form&amp;rsquo; or &amp;lsquo;shape.&lt;/p&gt;
&lt;p&gt;It refers to how people perceive visual elements in a given situation. Generally, we compare visual patterns and past experiences to make sense of what we see. We often perceive these elements as a single whole, rather than as separate parts. By connecting the elements, recognizing familiar shapes, sharing information, and filling in the gaps, we make sense of the overall picture.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;1-law-of-proximity&#34;&gt;1. Law of Proximity
&lt;/h3&gt;&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;http://localhost:1313/images/proximity.jpg&#34; alt=&#34;proximity&#34; width=&#34;70%&#34;&gt;
  &lt;p style=&#34;font-size: 12px; color: gray;&#34;&gt;Image Source: &lt;a href=&#34;https://www.toptal.com/designers/ui/gestalt-principles-of-design&#34; style=&#34;color: gray;&#34;&gt;By Cameron Chapman&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The &lt;strong&gt;Law of Proximity&lt;/strong&gt; descrives the phenomenon in which element that are close to each other are perceived and felt as a group.&lt;/p&gt;
&lt;p&gt;For example, in the left image, the dots don&amp;rsquo;t appear to be grouped. However, in the right image, the dots are closer together making them appear as three distinct groups. This &lt;strong&gt;proximity&lt;/strong&gt; can help in organizing related content by placing them close to each other.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;2-law-of-similarity&#34;&gt;2. Law of Similarity
&lt;/h3&gt;&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;http://localhost:1313/images/similarity.jpg&#34; alt=&#34;similarity&#34; width=&#34;60%&#34;&gt;
  &lt;p style=&#34;font-size: 12px; color: gray;&#34;&gt;Image Source: &lt;a href=&#34;https://www.toptal.com/designers/ui/gestalt-principles-of-design&#34; style=&#34;color: gray;&#34;&gt;By Cameron Chapman&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The &lt;strong&gt;Law of Similarity&lt;/strong&gt; means that objects with similar shpes, sizes, colors, or other attributes are perceived as part of the same group.&lt;/p&gt;
&lt;p&gt;In this example, all shapes are squares, but the difference in color causes our brain to group the green squares together and the gray squares together, even though they share the same shape. This shows how color similarity plays a key role in organizing visual elements into groups.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;3-law-of-closure&#34;&gt;3. Law of Closure
&lt;/h3&gt;&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;http://localhost:1313/images/closure.jpg&#34; alt=&#34;closure&#34; width=&#34;60%&#34;&gt;
  &lt;p style=&#34;font-size: 12px; color: gray;&#34;&gt;Image Source: &lt;a href=&#34;https://www.toptal.com/designers/ui/gestalt-principles-of-design&#34; style=&#34;color: gray;&#34;&gt;By Cameron Chapman&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The &lt;strong&gt;Law of Closure&lt;/strong&gt; refers to the tendency to perceive incomplete shapes as whole or complete. Our brain fills in the missing parts, allowing us to recognize an entire form even when elements are missing.&lt;/p&gt;
&lt;p&gt;In the image, some parts of the shapes are missing, but our brain automatically fills in the gaps, making us perceive the incomplete shapes as a complete form. This principle is commonly seen in logos or icons where parts of the design are missing, yet we still recognize the full shape.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;4-law-of-figure-ground&#34;&gt;4. Law of Figure-Ground
&lt;/h3&gt;&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;http://localhost:1313/images/figureground.jpg&#34; alt=&#34;figure-ground&#34; width=&#34;60%&#34;&gt;
  &lt;p style=&#34;font-size: 12px; color: gray;&#34;&gt;Image Source: &lt;a href=&#34;https://www.toptal.com/designers/ui/gestalt-principles-of-design&#34; style=&#34;color: gray;&#34;&gt;By Cameron Chapman&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The &lt;strong&gt;Law of Figure-Ground&lt;/strong&gt; describes how we distinguish an object (the figure) from its surrounding background (the ground). Our focus shifts between the object and the background, depending on what we are focusing on at any given moment.&lt;/p&gt;
&lt;p&gt;In this image, depending on where we focus, we may see either the foreground shapes as the figure or the background. This principle is often used in visual illusions, where the brain toggles between seeing two different images depending on whether it focuses on the figure or the ground.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;5-law-of-continuity&#34;&gt;5. Law of Continuity
&lt;/h3&gt;&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;http://localhost:1313/images/continuity.jpg&#34; alt=&#34;continutiy&#34; width=&#34;60%&#34;&gt;
  &lt;p style=&#34;font-size: 12px; color: gray;&#34;&gt;Image Source: &lt;a href=&#34;https://www.toptal.com/designers/ui/gestalt-principles-of-design&#34; style=&#34;color: gray;&#34;&gt;By Cameron Chapman&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The &lt;strong&gt;Law of Continuity&lt;/strong&gt; states that elements arranged on a line or curve are perceived as related or continuous. This principle explains how our eyes follow the smoothest path when interpreting visual elements.&lt;/p&gt;
&lt;p&gt;In this image, the red and gray dots form continuous curves. Even though they are separate dots, our brain perceives the curves as a single continuous path, demonstrating how continuity helps us organize visual elements in a flowing pattern.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;6-law-of-common-region&#34;&gt;6. Law of Common-region
&lt;/h3&gt;&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;http://localhost:1313/images/commonregion.jpg&#34; alt=&#34;common-region&#34; width=&#34;60%&#34;&gt;
  &lt;p style=&#34;font-size: 12px; color: gray;&#34;&gt;Image Source: &lt;a href=&#34;https://app.uxcel.com/lessons/law-of-common-region-899&#34; style=&#34;color: gray;&#34;&gt;Uxcel&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The &lt;strong&gt;Law of Common-region&lt;/strong&gt; states that elements located within the same boundary are perceived as part of a group. A visual boundary such as a box or a color background can create this perception of grouping, even if the elements are not physically close.&lt;/p&gt;
&lt;p&gt;In this image, the circles inside the box are perceived as a group because they share a common region (the box). Even though the circles outside the box are the same size, shape, and color, they are seen as separate because they are not within the same boundary.&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;applications-of-gestalt-laws-in-daily-life&#34;&gt;Applications of Gestalt laws in daily life
&lt;/h1&gt;&lt;h2 id=&#34;1-confusing-stairs&#34;&gt;&lt;em&gt;1. Confusing stairs&lt;/em&gt;
&lt;/h2&gt;&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;http://localhost:1313/images/gl_ex1.jpg&#34; alt=&#34;confusingstairs&#34; width=&#34;70%&#34;&gt;
  &lt;p style=&#34;font-size: 12px; color: gray;&#34;&gt;Image Source: &lt;a href=&#34;https://brightside.me/articles/15-designs-that-can-confuse-our-common-sense-809304/&#34; style=&#34;color: gray;&#34;&gt;Bright Side&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id=&#34;problem&#34;&gt;Problem
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Law of Continuity&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This pattern has consistent stripes, which causes our eyes to fail in distinguishing where the stairs end and begin, making the surface appear like a continuous plane. In situations where step differences need to be recognized, this pattern can create visual confusion, which poses a safety risk.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Law of Figure-Ground&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There is a lack of clear distinction between the figure (the stairs) and the background. As a result, it becomes harder to perceive the shape and depth of the stairs, and users may struggle to identify the height changes of the steps.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;solution&#34;&gt;Solution
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Clear Boundary Markings&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Adding a different colored stripe to the edges of each step can help clearly distinguish the boundary of the stairs. This will break the continuity and allow the steps to be seen as distinct units.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Enhancing Contrast&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To better separate the background and foreground, stronger color contrast can be applied to the steps. For example, using a brighter color on the edges of the stairs will help clearly define the steps, making it easier to perceive the changes in height.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;2-confusing-buttons-on-yes24&#34;&gt;&lt;em&gt;2. Confusing Buttons on YES24&lt;/em&gt;
&lt;/h2&gt;&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;http://localhost:1313/images/yes24.png&#34; alt=&#34;confusing buttons yes24&#34; width=&#34;60%&#34;&gt;
  &lt;p style=&#34;font-size: 12px; color: gray;&#34;&gt;Image Source: YES24&lt;/p&gt;
&lt;/div&gt;
*YES24 is a popular online bookstore in South Korea, where users can purchase physical books, eBooks, and other products.*
&lt;h3 id=&#34;problem-1&#34;&gt;Problem
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Law of Similarity&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;strong&gt;NPay Purchase&lt;/strong&gt; and &lt;strong&gt;One-Click Purchase&lt;/strong&gt; buttons look similar in color, which can confuse users. Since both buttons serve different functions but appear visually similar, users may accidentally choose the wrong payment option. Because Naver&amp;rsquo;s branding is strongly associated with green, the green color of the NPay button helps signify that it is a Naver-related payment method. However, the other buttons are also visually similar, which may lead to mistaken selections.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Law of Proximity&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;strong&gt;Buy&lt;/strong&gt;, &lt;strong&gt;Add to Cart&lt;/strong&gt;, &lt;strong&gt;NPay&lt;/strong&gt;, and &lt;strong&gt;One-Click Purchase&lt;/strong&gt; buttons are placed very close to each other, which can make it difficult for users to quickly differentiate between actions. When buttons with different functions are positioned without sufficient spacing, they are perceived as part of a single group, leading to potential confusion and misclicks.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;solution-1&#34;&gt;Solution
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Color Differentiation&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Keep the &lt;strong&gt;NPay button&lt;/strong&gt; green to signify its association with Naver, and use a different color for other purchase options, such as &lt;strong&gt;One-Click Purchase&lt;/strong&gt;. This will allow users to easily distinguish between payment methods based on color, reducing the chances of accidental selections.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Spacing Adjustments&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Increase the spacing between the &lt;strong&gt;Buy&lt;/strong&gt;, &lt;strong&gt;Add to Cart&lt;/strong&gt;, &lt;strong&gt;NPay&lt;/strong&gt;, and &lt;strong&gt;One-Click Purchase&lt;/strong&gt; buttons to create a clearer separation of actions. By adding more space, each button will stand out as a distinct option, making it easier for users to select the correct action without confusion.&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Set up Blog</title>
        <link>http://localhost:1313/post/lab1/setupblog/</link>
        <pubDate>Thu, 19 Sep 2024 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/post/lab1/setupblog/</guid>
        <description>&lt;img src="http://localhost:1313/images/hugo.jpg" alt="Featured image of post Set up Blog" /&gt;&lt;p&gt;Today, I&amp;rsquo;ll show you how to set up a blog using Hugo and deploy it on GitHub Pages. I&amp;rsquo;ll be working on a MacBook 💻 (Apple silicon).&lt;/p&gt;
&lt;h2 id=&#34;why-hugo&#34;&gt;Why Hugo?
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;No server-side code&lt;/strong&gt;: Hugo builds purely static files, so there is no need to manage any backend infrastructure.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fast to render&lt;/strong&gt;: Static sites are quick to render because they are pre-built into HTML before deployment.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;No dynamic content&lt;/strong&gt;: While this means no interactive elements like forms or real-time data updates, it also simplifies maintenance.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;No database&lt;/strong&gt;: Content is stored as files, not in a database, which reduces complexity and overhead.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Often more secure&lt;/strong&gt;: Fewer security vulnerabilities as there are no databases or server-side scripts to attack.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;No real-time UI&lt;/strong&gt;: Hugo sites don’t support real-time updates or interactions without additional tools.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Content is versioned&lt;/strong&gt;: With git, all content can be version controlled, providing an easy way to manage changes and rollbacks.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites
&lt;/h2&gt;&lt;p&gt;Before starting, make sure you have &lt;strong&gt;Homebrew&lt;/strong&gt; and Git installed on your MacBook. If you don&amp;rsquo;t have it, you can install it with the command below:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;/bin/bash -c &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;$(&lt;/span&gt;curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh&lt;span style=&#34;color:#66d9ef&#34;&gt;)&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;To check if Git is already installed, run the following command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git --version
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If Git is not installed, you can install it using Homebrew:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;brew install git
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once installed, verify it by running &lt;code&gt;git --version&lt;/code&gt; again to ensure everything is set up correctly.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;step-1-install-hugo&#34;&gt;Step 1: Install Hugo
&lt;/h2&gt;&lt;p&gt;To install Hugo, first you open the terminal and run this code&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;brew install hugo
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once the installation is complete, verify the installation by checking the Hugo version&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;hugo version
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#and if installed correctly, you should see an output similar to the following&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Hugo Static Site Generator v0.74.3/extended darwin/amd64 BuildDate: unknown
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h2 id=&#34;step-2-create-a-new-hugo-site&#34;&gt;Step 2: Create a New Hugo Site
&lt;/h2&gt;&lt;p&gt;Now that Hugo is installed, you can create your new blog. Navigate to the directory where you&amp;rsquo;d like to store your blog and create a new site with the command below:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;hugo new site &amp;lt;nameOftheSite&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Replace &lt;code&gt;nameOftheSite&lt;/code&gt; with your desired name for the blog site.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;step-3-add-a-theme&#34;&gt;Step 3: Add a Theme
&lt;/h2&gt;&lt;p&gt;To give your blog a proper design, you&amp;rsquo;ll need to add a theme. Hugo has a variety of themes available that you can browse and choose from.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Go to the &lt;a class=&#34;link&#34; href=&#34;https://themes.gohugo.io/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Hugo Themes website&lt;/a&gt; to browse available themes.&lt;/li&gt;
&lt;li&gt;Once you find a theme you like, click on it.&lt;/li&gt;
&lt;li&gt;On the theme&amp;rsquo;s page, there will be a &amp;ldquo;Download&amp;rdquo; button. When you click this button, you&amp;rsquo;ll be redirected to the theme&amp;rsquo;s GitHub repository.&lt;/li&gt;
&lt;li&gt;You should check the &lt;code&gt;ReadMe&lt;/code&gt; file on the GitHub repository for instructions on how to apply the theme.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;For example, I used the &lt;code&gt;hugo-theme-stack&lt;/code&gt; theme.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;First, initialize git in your project directory (ensure you are in the correct directory):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git init
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Then, add the theme as a git submodule:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git submodule add https://github.com/CaiJimmy/hugo-theme-stack/ themes/hugo-theme-stack
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now, open the &lt;code&gt;hugo.toml&lt;/code&gt; file and configure Hugo to use this theme by adding the following line:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-toml&#34; data-lang=&#34;toml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;theme&lt;/span&gt; = &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;hugo-theme-stack&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Each theme has different configuration options, so make sure to follow the specific instructions in the theme&amp;rsquo;s GitHub repository. You can also customize it further based on your needs.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;step-31-creating-new-posts&#34;&gt;Step 3.1: Creating New Posts
&lt;/h3&gt;&lt;p&gt;To create a new blog post, you can use the following Hugo command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;hugo new posts/&amp;lt;post-name&amp;gt;.md
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This will create a new Markdown file in the &lt;code&gt;content/posts/&lt;/code&gt; directory. You can then edit this file to write your post.&lt;/p&gt;
&lt;p&gt;For example:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;hugo new posts/my-first-post.md
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h2 id=&#34;step-4-create-a-new-github-repository&#34;&gt;Step 4: Create a New GitHub Repository
&lt;/h2&gt;&lt;p&gt;To deploy the site to GitHub Pages, you&amp;rsquo;ll need to connect it to a GitHub repository.&lt;/p&gt;
&lt;p&gt;First, create a new repository on your GitHub account.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;http://localhost:1313/images/newrepo.jpg&#34; alt=&#34;new_repository&#34; width=&#34;70%&#34;&gt;
&lt;/div&gt;
&lt;p&gt;&lt;em&gt;In my case, I created my repository with the name &lt;code&gt;&amp;lt;username&amp;gt;.github.io&lt;/code&gt; and left it public.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Once the repository is created, you&amp;rsquo;ll see a setup page. Here, you&amp;rsquo;ll find instructions on how to push your existing files to this repository.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;http://localhost:1313/images/quicksetup.png&#34; alt=&#34;new_repository&#34; width=&#34;70%&#34;&gt;
&lt;/div&gt;
&lt;p&gt;Go back to your terminal and connect your Hugo project to this GitHub repository by following the commands on the setup page (red box).&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;step-5-deploy-to-github-pages&#34;&gt;Step 5: Deploy to GitHub Pages
&lt;/h2&gt;&lt;p&gt;We&amp;rsquo;ll use GitHub Actions to automatically build and deploy the Hugo site to GitHub Pages whenever changes are pushed to the &lt;code&gt;main&lt;/code&gt; branch.&lt;/p&gt;
&lt;p&gt;I followed the steps indicated in the official Hugo documentation: &lt;a class=&#34;link&#34; href=&#34;https://gohugo.io/hosting-and-deployment/hosting-on-github/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Host on GitHub Pages&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Before you deploy it, you should change the &lt;code&gt;baseURL&lt;/code&gt; in the &lt;code&gt;hugo.toml&lt;/code&gt; file to your GitHub Pages URL.&lt;/p&gt;
&lt;p&gt;For example,&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-toml&#34; data-lang=&#34;toml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;baseURL&lt;/span&gt; = &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;https://jiwonyziyo.github.io/&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h2 id=&#34;step-6-access-your-site&#34;&gt;Step 6: Access Your Site
&lt;/h2&gt;&lt;p&gt;Once the GitHub Action finishes running, your blog will be deployed to GitHub Pages. You can check your blog by visiting the URL found in &lt;strong&gt;Settings &amp;gt; Pages&lt;/strong&gt;.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;http://localhost:1313/images/githuburl.jpg&#34; alt=&#34;github_url&#34; width=&#34;70%&#34;&gt;
&lt;/div&gt;
&lt;p&gt;To see the deployment status, you can also go to the &lt;strong&gt;Actions&lt;/strong&gt; tab in your GitHub repository. If everything went well, you&amp;rsquo;ll see that your site was successfully deployed.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;voilà--now-you-have-a-fully-functioning-personal-blog&#34;&gt;Voilà ! Now you have a fully functioning personal blog!
&lt;/h2&gt;&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;http://localhost:1313/images/website.jpg&#34; alt=&#34;website&#34; width=&#34;70%&#34;&gt;
&lt;/div&gt;
</description>
        </item>
        <item>
        <title>Set up Unity</title>
        <link>http://localhost:1313/post/lab2/setupunity/</link>
        <pubDate>Thu, 19 Sep 2024 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/post/lab2/setupunity/</guid>
        <description>&lt;img src="http://localhost:1313/images/unity.jpg" alt="Featured image of post Set up Unity" /&gt;&lt;p&gt;Today, I&amp;rsquo;ll show you the process of setting up Unity on a MacBook 💻.&lt;/p&gt;
&lt;h2 id=&#34;step-1-sign-up-for-a-unity-account&#34;&gt;Step 1: Sign Up for a Unity Account
&lt;/h2&gt;&lt;p&gt;To start using Unity, you&amp;rsquo;ll need to create an account. You can sign up on the &lt;a class=&#34;link&#34; href=&#34;https://id.unity.com/account/new&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Unity website&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Once your account is created, you will have access to Unity Hub.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;step-2-verify-student-status-for-unity-education-license&#34;&gt;Step 2: Verify Student Status for Unity Education License
&lt;/h2&gt;&lt;p&gt;Unity offers free premium access to students through the Unity Student Plan. Follow these steps to verify your student status:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Go to the &lt;a class=&#34;link&#34; href=&#34;https://store.unity.com/academic/unity-student&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Unity Student Plan page&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Click on &lt;strong&gt;Free Access: Post-Secondary&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Log in with the Unity account you created earlier. (&lt;em&gt;I used my personal Google account for registration but verified my student status using my school email.&lt;/em&gt;) During the verification process, you will need to provide your school information and expected graduation year.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;After verification, you will receive a completion email. Click on &lt;strong&gt;Redeem my Student plan&lt;/strong&gt; to finalize the process. Make sure to log in using the same email you used to sign up, not your school email.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;http://localhost:1313/images/redeem.png&#34; alt=&#34;redeem&#34; width=&#34;50%&#34;&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You will receive a license key via email, which you will use during the Unity installation process.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;http://localhost:1313/images/unitykey.png&#34; alt=&#34;unitykey&#34; width=&#34;50%&#34;&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;step-3-install-unity-hub&#34;&gt;Step 3: Install Unity Hub
&lt;/h2&gt;&lt;p&gt;Unity Hub is a central application for managing Unity installations, projects, and licenses. Here&amp;rsquo;s how to install Unity Hub on your MacBook:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Visit the &lt;a class=&#34;link&#34; href=&#34;https://unity.com/download&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Unity Hub download page&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Download and install Unity Hub for macOS.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Once installed, open Unity Hub and log in with your Unity account.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Now you need to add a license. Click on &lt;strong&gt;Preferences&lt;/strong&gt; in Unity Hub.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;http://localhost:1313/images/unitypreferences.png&#34; alt=&#34;preferences&#34; width=&#34;50%&#34;&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the menu on the left, go to &lt;strong&gt;Licenses&lt;/strong&gt; and click &lt;strong&gt;Add&lt;/strong&gt;. Select &lt;strong&gt;Activate with serial number&lt;/strong&gt;, then enter the license key you received via email.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;http://localhost:1313/images/unityadd.png&#34; alt=&#34;Add&#34; width=&#34;50%&#34;&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Your license, which grants one year of access to Unity, will now be active.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;http://localhost:1313/images/unitylicence.png&#34; alt=&#34;Licence&#34; width=&#34;50%&#34;&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;step-4-install-a-specific-version-of-unity&#34;&gt;Step 4: Install a Specific Version of Unity
&lt;/h2&gt;&lt;p&gt;For my HCI classes, I need to use a specific version of Unity rather than the latest one. Here’s how you can install a different version:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Open Unity Hub.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Go to the &lt;strong&gt;Installs&lt;/strong&gt; tab.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;http://localhost:1313/images/unityinstall.png&#34; alt=&#34;Install tab&#34; width=&#34;30%&#34;&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Click on &lt;strong&gt;Install Editor&lt;/strong&gt; to install a new Unity Editor version.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the &lt;strong&gt;Choose a version&lt;/strong&gt; section, select &lt;strong&gt;Archive&lt;/strong&gt; from the dropdown to see older versions of Unity.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;http://localhost:1313/images/unityversion.png&#34; alt=&#34;Version&#34; width=&#34;50%&#34;&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Select the required version for your class or project.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Choose the &lt;strong&gt;platform modules&lt;/strong&gt; you need (such as iOS, Android, WebGL, etc.).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Click &lt;strong&gt;Next&lt;/strong&gt;, and Unity will begin installing the selected version.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;step-5-set-up-your-first-unity-project&#34;&gt;Step 5: Set Up Your First Unity Project
&lt;/h2&gt;&lt;p&gt;Once Unity is installed, you can create your first project:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Open Unity Hub and go to the &lt;strong&gt;Projects&lt;/strong&gt; tab. Click &lt;strong&gt;New Project&lt;/strong&gt;.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;http://localhost:1313/images/unitynewproject.png&#34; alt=&#34;new_project&#34; width=&#34;50%&#34;&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Select a template (e.g., 2D, 3D, etc.) based on the type of project you want to build. Name your project and choose a location for it.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;http://localhost:1313/images/unitynewsetup.png&#34; alt=&#34;new_project&#34; width=&#34;50%&#34;&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Click &lt;strong&gt;Create&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Unity will set up the project, and you&amp;rsquo;ll be taken to the Unity Editor where you can start developing.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;voilà-&#34;&gt;Voilà !
&lt;/h2&gt;&lt;p&gt;Good luck, and enjoy building with Unity!&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
