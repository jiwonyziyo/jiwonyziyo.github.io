[{"content":"","date":"2024-10-14T00:00:00Z","permalink":"http://localhost:1313/post/lab3/unity3dapplication/","title":"Lab 3 - Unity 3D Application"},{"content":"Today, I\u0026rsquo;ll show you how to set up a blog using Hugo and deploy it on GitHub Pages. I\u0026rsquo;ll be working on a MacBook 💻 (Apple silicon).\nWhy Hugo? No server-side code: Hugo builds purely static files, so there is no need to manage any backend infrastructure. Fast to render: Static sites are quick to render because they are pre-built into HTML before deployment. No dynamic content: While this means no interactive elements like forms or real-time data updates, it also simplifies maintenance. No database: Content is stored as files, not in a database, which reduces complexity and overhead. Often more secure: Fewer security vulnerabilities as there are no databases or server-side scripts to attack. No real-time UI: Hugo sites don’t support real-time updates or interactions without additional tools. Content is versioned: With git, all content can be version controlled, providing an easy way to manage changes and rollbacks. Prerequisites Before starting, make sure you have Homebrew and Git installed on your MacBook. If you don\u0026rsquo;t have it, you can install it with the command below:\n/bin/bash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\u0026#34; To check if Git is already installed, run the following command:\ngit --version If Git is not installed, you can install it using Homebrew:\nbrew install git Once installed, verify it by running git --version again to ensure everything is set up correctly.\nStep 1: Install Hugo To install Hugo, first you open the terminal and run this code\nbrew install hugo Once the installation is complete, verify the installation by checking the Hugo version\nhugo version #and if installed correctly, you should see an output similar to the following Hugo Static Site Generator v0.74.3/extended darwin/amd64 BuildDate: unknown Step 2: Create a New Hugo Site Now that Hugo is installed, you can create your new blog. Navigate to the directory where you\u0026rsquo;d like to store your blog and create a new site with the command below:\nhugo new site \u0026lt;nameOftheSite\u0026gt; Replace nameOftheSite with your desired name for the blog site.\nStep 3: Add a Theme To give your blog a proper design, you\u0026rsquo;ll need to add a theme. Hugo has a variety of themes available that you can browse and choose from.\nGo to the Hugo Themes website to browse available themes. Once you find a theme you like, click on it. On the theme\u0026rsquo;s page, there will be a \u0026ldquo;Download\u0026rdquo; button. When you click this button, you\u0026rsquo;ll be redirected to the theme\u0026rsquo;s GitHub repository. You should check the ReadMe file on the GitHub repository for instructions on how to apply the theme. For example, I used the hugo-theme-stack theme.\nFirst, initialize git in your project directory (ensure you are in the correct directory):\ngit init Then, add the theme as a git submodule:\ngit submodule add https://github.com/CaiJimmy/hugo-theme-stack/ themes/hugo-theme-stack Now, open the hugo.toml file and configure Hugo to use this theme by adding the following line:\ntheme = \u0026#34;hugo-theme-stack\u0026#34; Each theme has different configuration options, so make sure to follow the specific instructions in the theme\u0026rsquo;s GitHub repository. You can also customize it further based on your needs.\nStep 3.1: Creating New Posts To create a new blog post, you can use the following Hugo command:\nhugo new posts/\u0026lt;post-name\u0026gt;.md This will create a new Markdown file in the content/posts/ directory. You can then edit this file to write your post.\nFor example:\nhugo new posts/my-first-post.md Step 4: Create a New GitHub Repository To deploy the site to GitHub Pages, you\u0026rsquo;ll need to connect it to a GitHub repository.\nFirst, create a new repository on your GitHub account.\nIn my case, I created my repository with the name \u0026lt;username\u0026gt;.github.io and left it public.\nOnce the repository is created, you\u0026rsquo;ll see a setup page. Here, you\u0026rsquo;ll find instructions on how to push your existing files to this repository.\nGo back to your terminal and connect your Hugo project to this GitHub repository by following the commands on the setup page (red box).\nStep 5: Deploy to GitHub Pages We\u0026rsquo;ll use GitHub Actions to automatically build and deploy the Hugo site to GitHub Pages whenever changes are pushed to the main branch.\nI followed the steps indicated in the official Hugo documentation: Host on GitHub Pages.\nBefore you deploy it, you should change the baseURL in the hugo.toml file to your GitHub Pages URL.\nFor example,\nbaseURL = \u0026#34;https://jiwonyziyo.github.io/\u0026#34; Step 6: Access Your Site Once the GitHub Action finishes running, your blog will be deployed to GitHub Pages. You can check your blog by visiting the URL found in Settings \u0026gt; Pages.\nTo see the deployment status, you can also go to the Actions tab in your GitHub repository. If everything went well, you\u0026rsquo;ll see that your site was successfully deployed.\nVoilà ! Now you have a fully functioning personal blog! ","date":"2024-10-12T00:00:00Z","image":"http://localhost:1313/images/hugo.jpg","permalink":"http://localhost:1313/post/lab1/setupblog/","title":"Lab 1 - Set up Blog"},{"content":"Today, I\u0026rsquo;ll show you the process of setting up Unity on a MacBook 💻.\nStep 1: Sign Up for a Unity Account To start using Unity, you\u0026rsquo;ll need to create an account. You can sign up on the Unity website.\nOnce your account is created, you will have access to Unity Hub.\nStep 2: Verify Student Status for Unity Education License Unity offers free premium access to students through the Unity Student Plan. Follow these steps to verify your student status:\nGo to the Unity Student Plan page.\nClick on Free Access: Post-Secondary.\nLog in with the Unity account you created earlier. (I used my personal Google account for registration but verified my student status using my school email.) During the verification process, you will need to provide your school information and expected graduation year.\nAfter verification, you will receive a completion email. Click on Redeem my Student plan to finalize the process. Make sure to log in using the same email you used to sign up, not your school email.\nYou will receive a license key via email, which you will use during the Unity installation process.\nStep 3: Install Unity Hub Unity Hub is a central application for managing Unity installations, projects, and licenses. Here\u0026rsquo;s how to install Unity Hub on your MacBook:\nVisit the Unity Hub download page.\nDownload and install Unity Hub for macOS.\nOnce installed, open Unity Hub and log in with your Unity account.\nNow you need to add a license. Click on Preferences in Unity Hub.\nIn the menu on the left, go to Licenses and click Add. Select Activate with serial number, then enter the license key you received via email.\nYour license, which grants one year of access to Unity, will now be active.\nStep 4: Install a Specific Version of Unity For my HCI classes, I need to use a specific version of Unity rather than the latest one. Here’s how you can install a different version:\nOpen Unity Hub.\nGo to the Installs tab.\nClick on Install Editor to install a new Unity Editor version.\nIn the Choose a version section, select Archive from the dropdown to see older versions of Unity.\nSelect the required version for your class or project.\nChoose the platform modules you need (such as iOS, Android, WebGL, etc.).\nClick Next, and Unity will begin installing the selected version.\nStep 5: Set Up Your First Unity Project Once Unity is installed, you can create your first project:\nOpen Unity Hub and go to the Projects tab. Click New Project.\nSelect a template (e.g., 2D, 3D, etc.) based on the type of project you want to build. Name your project and choose a location for it.\nClick Create.\nUnity will set up the project, and you\u0026rsquo;ll be taken to the Unity Editor where you can start developing.\nVoilà ! Good luck, and enjoy building with Unity!\n","date":"2024-10-12T00:00:00Z","image":"http://localhost:1313/images/unity.jpg","permalink":"http://localhost:1313/post/lab2/setupunity/","title":"Lab 2 - Set up Unity"},{"content":" why we use the virtual environement? (what is the purpose of the virutal -\u0026gt; is much easier to handle the each virtual ..) what kind of virtual envrionement? D:\\Users\\Studen\\Desktop\\kinect\u0026gt;.\\kinect\\student\\Scripts\\activate (student) D: \\Users\\Student\\Desktop\\kinect\u0026gt;\n(student) this means we are in the virtual envrionment\nvirtual -\u0026gt; create the folder / cmd / python -m venv .bonjour( . for hidden folder) / to activate: ..bonjour\\Scripts\\activate\nnumber of sensor -\u0026gt; number of kinect store in the data folder / images and depth\nrun python real_time.py : if you want to detect each face -\u0026gt; we can modify the code to draw a bounding box ror sth , detect the people faces or ,.. sdk -\u0026gt; kinect for windows : sample projects before, just check kinect toolkit /developper toolkit kinect toolkit and kinect sdk v1.8\nkinect fusion basics -\u0026gt; recontruct the walls, laptop, keyboard,.. we can even check the texture kinect head scanning -\u0026gt; create mesh -\u0026gt; obj and check in the unity depth basic -\u0026gt; black : really far away ","date":"2024-10-10T00:00:00Z","permalink":"http://localhost:1313/post/lab4/kinect/","title":"Lab 4 - Kinect"},{"content":"","date":"2024-10-08T14:46:43+02:00","permalink":"http://localhost:1313/post/ideate/ideate/","title":"Ideate"},{"content":"Prototype How you can apply the same thing this kind of prototype\nHeuristic evaluation - this should be done by experting fields by experts lab experiment/ Field study/ Survey -\u0026gt; are we missing any other research methods? in physics the \u0026ldquo;똘뜨스?\u0026rdquo;\ncreate some kind of hypothesis / it should be testable -\u0026gt; how bad or how good it is. importany to know about is variable.\ndifference bewtween two variables woth some examples ?\n독립변수, 종속변수\nfor example, imagine we work for gain the money. in this case the independent variable is the amount of work because this variable can change ourselves. the dependent variable is the salary. because the amount of the money we can earn is depends on the amount of work.\nindependent variable -\u0026gt; dependent variable -\u0026gt; control variable -\u0026gt; which affects the dependent variables\ncontrol variable -\u0026gt; ex. I drop a ball from certain height, i measure the bouncing height. (bouncing height -\u0026gt; dependent variable / certain height -\u0026gt; indepent variable / ball(itself) or the floor -\u0026gt; control variable. )\n","date":"2024-10-08T00:00:00Z","permalink":"http://localhost:1313/post/prototype/prototype/","title":"Lecture 8 - Prototype"},{"content":"HCI Researcher : Young-Ho Kim Current Role and Expertise Young-Ho Kim is a Lead Research Scientist at NAVER AI Lab and a prominent researcher in the field of HCI. His work spans across multiple domains, including Personal Health Informatics, Ubiquitous Computing (UbiComp), and Personal Data Visualization. By integrating his knowledge of computer science and visual communication design, Kim focuses on designing systems that enhance human-data interaction, especially in the context of self-tracking technologies.\nResearch Focus His current research emphasizes the development of flexible self-tracking systems that adapt to the needs, contexts, and preferences of individuals. His goal is to empower users to make informed decisions and behavioral changes through better data interaction. His approach often involves combining AI and Natural Language Processing (NLP) technologies, particularly Large Language Models (LLMs), to create intelligent self-trackers. These systems help users better understand and reflect on their behaviors and health data in everyday contexts.\nRecent Research Projects One of Kim\u0026rsquo;s notable ongoing projects includes:\nCareCall: A project designed to use LLMs for public health interventions. In this system, conversational AI assists socially isolated individuals by conducting regular check-up calls. The system not only gathers health metrics but also provides emotional support through empathetic conversations. The findings from this project highlight both the benefits and challenges of using LLM-driven systems in real-world public health contexts, such as balancing user expectations and handling the limitations of AI in personalization and memory. Another major project is:\nTextoshop: A novel tool that applies concepts from image editing to text manipulation. It allows users to engage in flexible and creative text editing by treating words and sentences similarly to how designers manipulate visual elements in software like Photoshop. Notable Contributions and Impact on HCI Kim\u0026rsquo;s work has significantly impacted how AI and human-centered design are integrated into real-world applications. By designing systems that bridge the gap between humans and data, he has contributed to making self-tracking technologies more accessible and adaptable to various user needs. His contributions to public health technology—especially in using AI to enhance social and emotional well-being—highlight the evolving role of HCI in addressing both technical and social challenges.\nKey Publications \u0026ldquo;Understanding the Benefits and Challenges of Deploying Conversational AI Leveraging Large Language Models for Public Health Intervention\u0026rdquo; (2023): This paper explores how LLM-based conversational AI systems can assist in public health interventions by monitoring and supporting socially isolated individuals.\nKim\u0026rsquo;s CareCall project paper\n\u0026ldquo;Textoshop: Interactions Inspired by Drawing Software to Facilitate Text Editing\u0026rdquo; (2024): In this paper, Kim and collaborators introduce a tool that reimagines text editing by applying principles of design software, enhancing user interaction with text through direct manipulation.\nTextoshop project paper\nAchievements Best Paper Award at CHI 2023 Best of CHI Honorable Mention in 2021 Recipient of the International Postdoc Fellowship from the National Research Foundation of Korea (2019) Conclusion Young-Ho Kim\u0026rsquo;s work at the forefront of HCI research continues to shape how we think about user interaction with data and technology. His contributions to flexible self-tracking systems and AI-driven health interventions exemplify the power of integrating advanced AI with human-centered design, making significant strides in both public health and everyday technology interactions.\n","date":"2024-10-03T00:00:00Z","image":"http://localhost:1313/images/researcher.png","permalink":"http://localhost:1313/post/hciresearcher/researcher/","title":"Lecture 4 - HCI Researcher"},{"content":"Reflections on Ivan Sutherland\u0026rsquo;s \u0026ldquo;The Ultimate Display\u0026rdquo; In his seminal work \u0026ldquo;The Ultimate Display\u0026rdquo; (1965), Ivan Sutherland laid the foundation for what we now recognize as virtual and augmented reality (VR/AR). His visionary ideas foreshadowed many technologies that are prevalent today, and he also introduced concepts that continue to guide future developments in human-computer interaction and immersive experiences.\nCurrent Realizations of Sutherland’s Vision 1. Virtual Reality (VR) and Augmented Reality (AR) One of Sutherland\u0026rsquo;s most notable predictions was the creation of virtual worlds indistinguishable from reality. He envisioned a computer-generated environment that could simulate reality with such fidelity that users would be unable to tell the difference between the virtual and the real. Today, VR headsets like the Oculus Rift, HTC Vive, and PlayStation VR offer immersive environments where users can interact with 3D worlds. Augmented Reality (AR) systems, like those seen in Microsoft’s HoloLens and mobile apps such as Pokémon Go, overlay digital information onto the real world, blending the virtual with the physical.\n2. Haptic Feedback and Tactile Displays Sutherland also imagined the possibility of interacting with virtual objects in a way that mimicked their real-world counterparts, suggesting that \u0026ldquo;a chair displayed in such a room would be good enough to sit in.\u0026rdquo; This idea has come to life through haptic technology and force feedback systems, which allow users to “feel” virtual objects. Today’s haptic gloves and vests can simulate touch, texture, and resistance, enhancing the immersive experience in virtual worlds.\n3. Interactive Graphics and Real-Time Simulation Sutherland\u0026rsquo;s work directly influenced the development of interactive computer graphics. He envisioned dynamic environments where users could manipulate objects in real-time. Modern computer graphics, powered by game engines such as Unreal Engine and Unity, allow for the creation of highly interactive and realistic simulations. Video games, 3D modeling software, and even training simulations for fields like medicine and aviation use these technologies.\nWhat Could Become Reality in the Future? 1. Full Sensory Immersion Sutherland hinted at the possibility of stimulating all human senses in virtual environments, suggesting that a truly ultimate display would engage sight, sound, touch, and possibly even taste and smell. While current VR systems focus primarily on visual and auditory experiences, the future could see the development of technology that simulates all senses. Olfactory displays (devices that emit smells) and gustatory technology (taste simulation) are still in early research stages, but with advancements in neuroscience and sensory technology, a fully immersive multisensory virtual world may become possible.\n2. Brain-Computer Interfaces (BCI) Another aspect of Sutherland\u0026rsquo;s vision was the direct interaction between the human brain and computers. While he did not explicitly predict brain-computer interfaces (BCI), his ideas about creating seamless interaction between humans and machines suggest that this could be the next step. Current research into BCI technology, such as Neuralink by Elon Musk, explores the potential of controlling computers and virtual environments using only thoughts. This could lead to a future where users can fully immerse themselves in virtual worlds without the need for physical hardware.\n3. Shared Virtual Worlds and Digital Communities Sutherland\u0026rsquo;s ultimate display also hinted at the potential for shared virtual experiences, where multiple users could interact with each other in a virtual space. This concept is becoming a reality with the advent of the metaverse, a digital universe where users can socialize, work, and play in a shared, persistent environment. Companies like Meta (formerly Facebook) and Epic Games are investing heavily in this area, aiming to create a connected virtual world where people can live out alternative realities together.\nConclusion Ivan Sutherland’s “The Ultimate Display” was groundbreaking in its vision of a future where humans and computers interact in ways that were unimaginable at the time. Many of his predictions have become reality, including virtual environments, interactive graphics, and haptic feedback systems. His vision continues to inspire the next generation of researchers and technologists, pushing the boundaries of VR, AR, and BCI technologies.\nAs we move forward, it’s clear that Sutherland’s concept of a fully immersive, multisensory, and interactive digital world remains the ultimate goal of HCI, and his work will continue to shape the future of the field.\nSource: The Ultimate Display by Ivan Sutherland\n","date":"2024-10-03T00:00:00Z","image":"http://localhost:1313/images/ivan.jpg","permalink":"http://localhost:1313/post/readivan/ultimatedisplay/","title":"Lecture 5 - Ultimate Display by Ivan Sutherland "},{"content":"Input Devices and Interaction Paradigms: The Leap Motion Controller Overview of the Leap Motion Controller The Leap Motion Controller, introduced in 2013, was a gesture-based input device that aimed to revolutionize how we interact with computers by allowing users to control on-screen elements using hand and finger movements. Unlike traditional input devices like the mouse or keyboard, Leap Motion provided a touch-free, 3D interaction experience. It was designed to track the movement of users’ hands in real-time, capturing detailed motions within an interaction space of about 8 cubic feet.\nClassification: Gesture-Based User Interface The Leap Motion falls under the category of Gesture-Based User Interface (GBUI). This type of user interface allows users to interact with a computer through body movements—specifically, hand and finger gestures. GBUI is a part of Natural User Interfaces (NUI), which focus on intuitive and natural interaction techniques.\nWhy the Leap Motion Failed to Succeed While Leap Motion was exciting in its innovation and garnered significant media attention upon launch, it ultimately did not achieve widespread adoption. Several reasons contribute to its failure to succeed:\n1. Limited Use Cases Although the Leap Motion device was revolutionary in concept, it struggled to find compelling real-world applications. Its primary use cases were limited to gaming, 3D modeling, and a few experimental applications. The niche appeal of these areas limited the audience. Most users still found traditional input methods (like the mouse and keyboard) more convenient and efficient for daily tasks.\n2. Accuracy and Tracking Issues While Leap Motion promised precise hand tracking, many users reported inconsistent accuracy and latency issues, especially when their hands moved too quickly or left the designated interaction zone. This inconsistency frustrated users and made it difficult to rely on for precise tasks, such as 3D design or professional workflows.\n3. Lack of Developer Support The success of input devices heavily depends on third-party developer support to create applications and software that leverage the technology. In Leap Motion’s case, there was a lack of sufficient developer support to build a robust ecosystem of applications. Many developers found the Leap Motion SDK challenging to work with, and without compelling applications, users had little incentive to adopt the device.\n4. Competition with VR and AR Technologies At the time of its launch, Virtual Reality (VR) and Augmented Reality (AR) were becoming more popular, with devices like the Oculus Rift and Microsoft HoloLens capturing the attention of both developers and users. Leap Motion tried to pivot toward VR, offering integration with VR headsets, but it was too late to compete effectively in the growing immersive tech space.\nCould It Succeed in the Future? Though the Leap Motion Controller did not succeed in its initial form, the technology behind it could find new life in the future with advancements in machine learning and computer vision. Gesture-based interfaces are becoming increasingly important in augmented reality (AR) and virtual reality (VR) environments, where hands-free interaction is critical.\nFor instance, Leap Motion\u0026rsquo;s acquisition by Ultrahaptics in 2019 (now Ultraleap) suggests there is still interest in developing touchless interaction technologies. By combining Leap Motion\u0026rsquo;s gesture recognition technology with haptic feedback systems, there is potential for future devices that offer more immersive and precise touchless experiences in healthcare, industrial design, or remote collaboration.\nConclusion The Leap Motion Controller was a visionary product that failed to achieve widespread success due to a lack of compelling use cases, tracking issues, and insufficient developer support. However, with the growing demand for gesture-based interaction in VR/AR environments, and the potential for improved accuracy through new technologies, the underlying concepts of Leap Motion may still play a key role in future human-computer interaction paradigms.\nSources:\nLeap Motion Controller Overview Why Leap Motion Failed Ultrahaptics Acquires Leap Motion ","date":"2024-10-03T00:00:00Z","permalink":"http://localhost:1313/post/devices/inputdevices/","title":"Lecture 6 - Input Devices and Interaction Paradigm"},{"content":"Image Source: Anant Kadiyala Affordance gives users a visual hint on what actions they can take.\nFor example, when we see a button, we instinctively want to press it, or when we see a switch, we feel like pulling it. On an app or website, a rectangular box with a border makes us think we can click and type into it. These cues play into human psychology.\nThus, affordance is a crucial element of visual user interfaces. The clearer the visual cues are, the less ambiguity there is for the user to understand what action is expected.\nIn mobile environments, the importance of affordance becomes even more pronounced, as the small screen size and limited space make it harder for affordance to be as visually obvious. Therefore, clear affordance is critical to ensure smooth user interaction on mobile devices.\nGood case - Traffic lights that show the remaining time Image Source: Busan News\n1. Clear Information Display\nTraffic lights that show the remaining time provide users with clear information on how much time is left before the signal changes. Pedestrians can easily determine whether they have enough time to cross the street, and drivers can predict when the signal will switch. This intuitive display helps users make more informed and safer decisions. 2. Behavior Guidance\nThe countdown timer allows both pedestrians and drivers to act accordingly. Pedestrians can choose to cross quickly if there is enough time, or wait for the next signal if time is running out. This guides user behavior and enhances traffic safety by reducing risky actions. 3. Meeting User Expectations\nIn addition to simply showing red or green lights, the countdown timer allows users to predict signal changes more accurately. This design meets user expectations by providing precise information, reducing stress at intersections, and improving overall traffic flow. Bad case - Keyboard Image Source: https://www.clien.net/service/board/park/16744923\nThe placement of a power button above the delete key on a keyboard is a bad example of affordance.\n1. Risk of Accidental Use\nWhen the power button is located too close to the delete key, users are more likely to press it accidentally, potentially shutting down the system unintentionally. The functions are too distinct to be placed so closely. 2. Functional Mismatch\nThe delete key is frequently used, whereas the power button is not. Placing such an important function near a less frequently used key increases the risk of errors and reduces the overall usability. 3. Contrary to User Expectations\nUsers do not expect two very different functions to be positioned so closely together. This placement disrupts the intuitive understanding of how the keyboard should work. Solutions 1. Relocate the Power Button\nThe most straightforward solution is to move the power button to a location further away from frequently used keys like the delete key. For example, the power button could be placed at the right-top corner of the keyboard, near the function keys or in a separate, more isolated area where it’s less likely to be pressed by accident. 2. Add a Confirmation Step\nImplementing a confirmation step when the power button is pressed would prevent accidental shutdowns. For instance, instead of instantly shutting down the system, the button could trigger a prompt asking the user to confirm the action. ","date":"2024-09-19T00:00:00Z","image":"http://localhost:1313/images/affordance.jpg","permalink":"http://localhost:1313/post/affordances/affordances/","title":"Lecture 1 - Affordances"},{"content":"Image Source: Gestalt Principles\nGestalt is a German word meaning \u0026lsquo;form\u0026rsquo; or \u0026lsquo;shape.\nIt refers to how people perceive visual elements in a given situation. Generally, we compare visual patterns and past experiences to make sense of what we see. We often perceive these elements as a single whole, rather than as separate parts. By connecting the elements, recognizing familiar shapes, sharing information, and filling in the gaps, we make sense of the overall picture.\n1. Law of Proximity Image Source: By Cameron Chapman\nThe Law of Proximity descrives the phenomenon in which element that are close to each other are perceived and felt as a group.\nFor example, in the left image, the dots don\u0026rsquo;t appear to be grouped. However, in the right image, the dots are closer together making them appear as three distinct groups. This proximity can help in organizing related content by placing them close to each other.\n2. Law of Similarity Image Source: By Cameron Chapman\nThe Law of Similarity means that objects with similar shpes, sizes, colors, or other attributes are perceived as part of the same group.\nIn this example, all shapes are squares, but the difference in color causes our brain to group the green squares together and the gray squares together, even though they share the same shape. This shows how color similarity plays a key role in organizing visual elements into groups.\n3. Law of Closure Image Source: By Cameron Chapman\nThe Law of Closure refers to the tendency to perceive incomplete shapes as whole or complete. Our brain fills in the missing parts, allowing us to recognize an entire form even when elements are missing.\nIn the image, some parts of the shapes are missing, but our brain automatically fills in the gaps, making us perceive the incomplete shapes as a complete form. This principle is commonly seen in logos or icons where parts of the design are missing, yet we still recognize the full shape.\n4. Law of Figure-Ground Image Source: By Cameron Chapman\nThe Law of Figure-Ground describes how we distinguish an object (the figure) from its surrounding background (the ground). Our focus shifts between the object and the background, depending on what we are focusing on at any given moment.\nIn this image, depending on where we focus, we may see either the foreground shapes as the figure or the background. This principle is often used in visual illusions, where the brain toggles between seeing two different images depending on whether it focuses on the figure or the ground.\n5. Law of Continuity Image Source: By Cameron Chapman\nThe Law of Continuity states that elements arranged on a line or curve are perceived as related or continuous. This principle explains how our eyes follow the smoothest path when interpreting visual elements.\nIn this image, the red and gray dots form continuous curves. Even though they are separate dots, our brain perceives the curves as a single continuous path, demonstrating how continuity helps us organize visual elements in a flowing pattern.\n6. Law of Common-region Image Source: Uxcel\nThe Law of Common-region states that elements located within the same boundary are perceived as part of a group. A visual boundary such as a box or a color background can create this perception of grouping, even if the elements are not physically close.\nIn this image, the circles inside the box are perceived as a group because they share a common region (the box). Even though the circles outside the box are the same size, shape, and color, they are seen as separate because they are not within the same boundary.\nApplications of Gestalt laws in daily life 1. Confusing stairs Image Source: Bright Side\nProblem Law of Continuity\nThis pattern has consistent stripes, which causes our eyes to fail in distinguishing where the stairs end and begin, making the surface appear like a continuous plane. In situations where step differences need to be recognized, this pattern can create visual confusion, which poses a safety risk. Law of Figure-Ground\nThere is a lack of clear distinction between the figure (the stairs) and the background. As a result, it becomes harder to perceive the shape and depth of the stairs, and users may struggle to identify the height changes of the steps. Solution Clear Boundary Markings\nAdding a different colored stripe to the edges of each step can help clearly distinguish the boundary of the stairs. This will break the continuity and allow the steps to be seen as distinct units. Enhancing Contrast\nTo better separate the background and foreground, stronger color contrast can be applied to the steps. For example, using a brighter color on the edges of the stairs will help clearly define the steps, making it easier to perceive the changes in height. 2. Apple Recent call Problem Law of Proximity\nIn this interface, the name and the call action are perceived as a single unit or function because of how the interaction works (tapping on the name directly initiates a call.) Users expect tapping on the name to display contact details, but because the call action is implicitly tied to this tap, they accidentally initiate calls. The proximity of the name and the action (or the lack of distinction between the two actions) leads to confusion. Solution Add a confirmation dialog\nAfter tapping a contact name, ask the user, \u0026ldquo;Do you want to make a call?\u0026rdquo; This would help prevent accidental calls. Improve visual cues\nClearly distinguish the action of viewing contact details from initiating a call. For example, a separate button for calling that’s visually distinct from the contact\u0026rsquo;s name would better guide the user’s actions. ","date":"2024-09-19T00:00:00Z","image":"http://localhost:1313/images/gestalt.jpg","permalink":"http://localhost:1313/post/gestalt-laws/gestaltlaw/","title":"Lecture 2 - Gestalt Law"},{"content":"Image Source: By John O'Malley Image Source: By Krisztina Szerovay\nDark patterns are UI/UX design techniques intentionally crafted to exploit human psychology and trick users into doing things they don’t necessarily want to do. The British UX designer Harry Brignull, who first coined the term, described dark patterns as follows\n“A carefully crafted user interface designed to trick users into performing actions such as signing up for insurance or signing up for recurring bills.”\nIn other words, dark patterns refer to UI/UX that is cleverly and intentionally designed to elicit certain actions from users, whether they want to or not. These designs often serve business goals, such as increasing subscriptions or data collection, but they do so at the expense of the user’s autonomy, leading to unintended actions like accidental purchases or subscriptions.\nTypes of Dark Patterns Design Here are five types of dark patterns commonly seen\nNagging Image Source: ResearchGate\nNagging occurs when an app or website repeatedly interrupts the user’s progress with prompts or messages, draining their time and attention. These interruptions can make the user feel pressured or annoyed, leading them to eventually agree to the message or request—even if it’s not what they want—just to move forward. If the interruptions happen frequently, the user might decide that giving in to the prompt is easier than continuing to dismiss it. This pattern is commonly seen in requests to subscribe to premium services, allow notifications, or share personal data, and it can result in a frustrating user experience. ","date":"2024-09-19T00:00:00Z","image":"http://localhost:1313/images/darkpatterndesign.jpg","permalink":"http://localhost:1313/post/dark-design-patterns/darkdesignpattern/","title":"Lecture 3 - Dark Design Patterns"}]