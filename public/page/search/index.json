[{"content":"Sense Boost Summary\nIdea: Sense Boost\nSense Boost is an AR glasses concept designed for individuals with hearing impairments. It visualizes surrounding sounds in real time, translating sound intensity and direction into visual effects displayed on the screen. This allows users to perceive sound information through sight rather than hearing, especially for specific sounds like conversations or alerts.\nPrototype\nWith Sense Boost AR glasses, users can see sounds visually displayed in front of them. For instance, if someone on the left says, ‚ÄúI‚Äôm tired,‚Äù the glasses show sound waves from the left, with the spoken text ‚ÄúI‚Äôm tired‚Äù displayed beneath. The size of the sound wave varies depending on the volume of the sound ‚Äî louder sounds appear as larger waves, while softer, distant sounds are represented by smaller, thinner waves, helping the user gauge both volume and proximity.\nEvaluation\nFeedback and Suggestions:\nNeed for Sound Filtering Options\nWhen multiple sounds are displayed simultaneously, the screen can become cluttered. It was suggested to have options to filter specific types of sounds, such as displaying only conversational sounds or prioritizing important alert sounds.\nDistinguishing Important Sounds\nWhen multiple people are speaking or when there are various sounds, it may be challenging to discern which sound is important. Adding an option to selectively emphasize crucial sounds could enhance usability.\nPotential Eye Strain with Extended Use\nSince the AR display continuously presents information in front of the user‚Äôs eyes, it could lead to eye strain or discomfort over time. To address this, an alternative approach, such as a smartphone app, could reduce this strain, allowing users to view sounds only when needed.\nSummary of Improvement Suggestions: Enhancements could include sound filtering, prioritization of important sounds, and exploring alternative devices for viewing.\nIn summary, Sense Boost is an innovative AR tool designed to assist hearing-impaired individuals by allowing them to see sounds visually. This initial feedback highlights valuable areas for potential improvements, making it even more user-centric.\n","date":"2024-10-27T00:00:00Z","permalink":"http://localhost:1313/post/ideate/ideateprototypeevaluation/","title":"Idea \u0026 Prototype \u0026 Evaluation"},{"content":"What is ubiquitous computing? Ubiquitous computing refers to the concept of computers naturally blending into people\u0026rsquo;s daily environments, allowing them to interact with computers anytime, anywhere. It aims to realize \u0026ldquo;invisible\u0026rdquo; computing by embedding computers into various objects and spaces around them, rather than limiting them to specific devices.\nAre we already living in a world of ubiquitous computing? Yes, we are already living in a world of ubiquitous computing to some extent.\nFor example, technologies such as calling an elevator in advance through a smartphone before leaving the house and receiving automatic notifications at home when a car enters an underground parking lot are becoming widespread. These smart home technologies are creating an environment where IoT and AI-based systems automatically respond to the user\u0026rsquo;s needs. What aspects of Weiser‚Äôs vision have already happened? Weiser‚Äôs vision of invisible, seamless computing is becoming more real with some of today‚Äôs technologies. Here are two examples that illustrate how close we are:\nAmazon Go\nAmazon Go‚Äôs Checkout-Free Shopping\nAmazon Go stores use sensors and AI to detect items shoppers pick up and automatically charge them when they leave the store, eliminating the need for a checkout line. This technology, which lets people simply ‚Äúgrab and go,‚Äù reflects Weiser‚Äôs vision of technology that integrates seamlessly into our routines without obvious intervention. Apple Watch Fall Detection and Emergency Call\nWhen an Apple Watch detects a hard fall, it automatically sends an alert to emergency services if the user doesn‚Äôt respond within a few seconds. This kind of technology demonstrates how computing can act almost invisibly in the background, offering potentially life-saving help without the user actively engaging with it. What Needs to Happen for a More Ubiquitous Computing World? While ubiquitous computing offers convenience and integration, it also brings serious privacy and security risks. Here are some examples:\nPersonal Surveillance and Privacy Risks\nRecently in South Korea, a widespread hacking incident involving apartment intercom cameras led to residents‚Äô private lives being recorded and leaked online. This incident illustrates how vulnerable connected devices in personal spaces can be to unauthorized access. As we add more cameras and sensors into homes, offices, and public areas, the risk of such breaches grows. Without robust security measures, these technologies can easily turn from helpers into privacy threats. Data Hacking and Identity Theft\nAs more devices are interconnected, they create additional access points for hackers. Wearables, smart appliances, and IoT devices are often less secure, making them easy targets for cyberattacks. A single breach could expose multiple data types across devices, increasing risks of identity theft and financial loss. This example highlights that while ubiquitous computing makes life more convenient, it also requires rigorous security standards to protect users. We need strong encryption, secure data storage, and regular software updates to mitigate these risks. Only then can we enjoy the benefits of a connected world without sacrificing privacy and security.\nSource: The Computer for the 21st Century by Mark Weiser\n","date":"2024-10-20T00:00:00Z","image":"http://localhost:1313/images/ubiquitos.jpeg","permalink":"http://localhost:1313/post/21stcentury/thecomputeforthe21stcentury/","title":"The Compute For the 21st Century"},{"content":"The shape of smartphones is continuously evolving, driven by technological advancements and shifting consumer demands. From the earliest cell phones to today‚Äôs smartphones, designs have undergone significant changes in both form and functionality, with further innovations anticipated in the future.\n1. Cell Phone Shapes Before Smartphones Early cell phones began as large, heavy, rectangular ‚Äúbar‚Äù shapes focused primarily on basic calling functions. Later, flip and slide designs emerged, offering screen protection and larger keypads, enhancing ease of use.\n2. The Earliest Forms of Smartphones Blackberry Bold 9650\niPhone 3gs\nBlackBerry: BlackBerry featured a compact, rectangular design with a physical keyboard and small screen, appealing especially to business users. The physical keyboard enabled fast and accurate typing, making it ideal for email and messaging.\niPhone: iPhone was revolutionary in its fully touchscreen rectangular design. By replacing the physical keyboard with a touch interface and adding a single home button, the iPhone‚Äôs minimalist design set the standard for app-centric usage, significantly impacting the shape of future smartphones.\n3. Why Physical Keyboards Disappeared Maximizing Screen Size\nPhysical keyboards occupied valuable screen space, limiting the display size on smartphones. With the adoption of touchscreens, the entire front surface could be utilized as a display. Enabling Diverse Input Methods\nTouchscreens support not only typing but also various gesture inputs such as multi-touch, swipe, and zooming. This allows users to interact with smartphones in a more intuitive way, going beyond simple text input. 4. The Future Shape of Smartphones Future smartphones are expected to take on more flexible shapes, like foldable and rollable designs, in response to evolving user needs.\nFoldable Smartphones: These devices provide a large display but fold up compactly to fit in a pocket, ideal for users who want both portability and a larger screen when needed. For instance, devices like the Galaxy Fold offer a large screen for immersive experiences but fold down for easy portability.As technology advances, we might even see multi-fold designs that can be folded multiple times, offering even larger displays while maintaining compactness for daily carry.\nRollable Smartphones: Rollable smartphones, which expand when additional screen space is required, address the demand for larger screens without compromising on compactness when not in use.\nConclusion Ultimately, smartphones have developed into more portable forms because they have become essential items we carry everywhere. At the same time, users still desire larger screens, and meeting both needs has led to concepts like foldable designs that can be carried compactly or rollable screens that expand when needed. This evolution reflects our desire for smartphones that are lighter, more compact, and easier to carry around, while still offering a larger display when required.\n","date":"2024-10-20T00:00:00Z","image":"http://localhost:1313/images/shapeofsmart.jpg","permalink":"http://localhost:1313/post/shapeofsmart/shapeofsmartphone/","title":"The Shape of Smartphones"},{"content":"VR, AR, and MR VR (Virtual Reality): VR immerses users in a completely virtual environment, isolating them from the real world. Users experience a 100% simulated environment. (e.g. VR museum)\nAR (Augmented Reality): AR overlays digital information onto the real world. Users see information or graphics layered onto their actual surroundings. (e.g. Pok√©mon GO)\nMR (Mixed Reality): Like AR, MR is based on the real world, but it allows digital objects to interact with physical surroundings. For example, a virtual character might sit on a real desk, blending naturally into the environment.\nFuture MR App Store: Most Downloaded Apps (Entertainment Focus) Olympic Sports Experience App\nFunction: Allows users to experience Olympic events in MR, such as ski jumping or competitive swimming, as if they‚Äôre truly participating. Example: We can perform a ski jump motion in their living room, and the MR display creates the scene of an actual jump, complete with timing and visuals. Popularity Reason: People can safely and thrillingly explore various sports without risk, appealing to adventure-seekers. Virtual Concerts and Festivals\nFunction: Enables users to experience concerts or virtual festivals at home, with artists appearing as if they‚Äôre performing live in front of them. Example: In a virtual concert setting, users can enjoy the music up close and interact with the crowd, feeling like they‚Äôre part of the event. Popularity Reason: For fans who can‚Äôt attend live events, this app provides a unique chance to connect closely with favorite artists. Conclusion One of MR‚Äôs main strengths is that it allows people to experience things that would otherwise require a lot of time and money. This accessibility increase equality, giving everyone the chance to explore, learn, and participate in experiences that were previously limited. Thanks to this, apps that make diverse experiences more accessible are likely to become highly popular in the future MR app store.\nMoreover, MR\u0026rsquo;s applications extend far beyond entertainment. In education, MR can immerse students in interactive learning environments, from virtual science labs to historical reenactments. In industry, MR is invaluable for training, allowing professionals to practice skills in realistic yet safe virtual settings, such as operating complex machinery or conducting medical procedures.\n","date":"2024-10-20T00:00:00Z","image":"http://localhost:1313/images/mr.jpg","permalink":"http://localhost:1313/post/mrapp/mrapp/","title":"VR \u0026 AR \u0026 MR"},{"content":"What is Kinect? Kinect is a motion-sensing device used for recognizing human movement. Kinect includes a 3D depth camera, RGB camera, and microphone array, making it highly effective for tracking body positions and movements. Today, Kinect is widely used in research fields and various HCI projects beyond gaming.\nPurpose of a Virtual Environment Using a virtual environment helps in managing project-specific dependencies independently. Each project can have its own set of libraries and versions, preventing conflicts with other projects and streamlining workflow.\nTo create and activate a virtual environment Create the virtual environment folder In the project directory, such as D:\\Users\\Student\\Desktop\\kinect, create a virtual environment using a hidden folder python -m venv .bonjour Activate the virtual environment Use this command to activate it .\\.bonjour\\Scripts\\activate Check if activated If activated successfully, the prompt will display (student) before the directory path, indicating that the virtual environment is in use. Kinect and Sensor Configuration Kinect has multiple sensors that capture both image and depth data simultaneously. Data is stored in the project‚Äôs data folder, with image and depth data organized separately.\nExamples of Kinect Functionalities Kinect Fusion Head Scanning As shown in the image above, Kinect Fusion allows 3D head scanning, which reconstructs a detailed 3D model of the user\u0026rsquo;s head. Users can adjust parameters like \u0026ldquo;Volume Max Integration Weight\u0026rdquo; and \u0026ldquo;Volume Voxel Resolution\u0026rdquo; to control the detail and quality of the scan. This functionality is useful for applications that require 3D head models, which can be exported as .STL, .OBJ, or .PLY files for use in other software. Depth Sensing and Mapping This image illustrates Kinect‚Äôs depth sensing capabilities, where objects are visualized based on their distance from the sensor. Black areas represent regions farthest from the Kinect, while closer areas appear in lighter shades. This depth map allows for spatial awareness, essential for applications like gesture recognition and object tracking. Face Tracking This image showcs Kinect‚Äôs face tracking functionality. Here, Kinect detects the user‚Äôs face and maps key facial features using a network of lines, making it ideal for applications requiring facial recognition, facial expression analysis, or real-time face-driven animations. Real-Time Data Collection and Processing To initiate real-time data collection, run the real_time.py script:\npython real_time.py This script enables real-time detection of faces and bodies. It can be modified to add functionalities such as drawing bounding boxes around detected faces or adding other interactive elements.\nKinect SDK and Toolkit In our project, we‚Äôre using the Kinect SDK and the Kinect Developer Toolkit. These tools give us access to Kinect‚Äôs main features, like motion tracking, depth sensing, and 3D scanning, making it easier to work with the Kinect sensor. The SDK includes useful sample projects, such as Skeleton Basics for tracking body movements.\nAnother example of Kinect - Skeleton Basics ","date":"2024-10-10T00:00:00Z","image":"http://localhost:1313/images/kinectimage.png","permalink":"http://localhost:1313/post/lab4/kinect/","title":"Kinect"},{"content":"HCI Researcher : Young-Ho Kim Current Role and Expertise Young-Ho Kim is a Lead Research Scientist at NAVER AI Lab and a prominent researcher in the field of HCI. His work spans across multiple domains, including Personal Health Informatics, Ubiquitous Computing (UbiComp), and Personal Data Visualization. By integrating his knowledge of computer science and visual communication design, Kim focuses on designing systems that enhance human-data interaction, especially in the context of self-tracking technologies.\nResearch Focus His current research emphasizes the development of flexible self-tracking systems that adapt to the needs, contexts, and preferences of individuals. His goal is to empower users to make informed decisions and behavioral changes through better data interaction. His approach often involves combining AI and Natural Language Processing (NLP) technologies, particularly Large Language Models (LLMs), to create intelligent self-trackers. These systems help users better understand and reflect on their behaviors and health data in everyday contexts.\nRecent Research Projects CareCall: A project designed to use LLMs for public health interventions. In this system, conversational AI assists socially isolated individuals by conducting regular check-up calls. The system not only gathers health metrics but also provides emotional support through empathetic conversations. The findings from this project highlight both the benefits and challenges of using LLM-driven systems in real-world public health contexts, such as balancing user expectations and handling the limitations of AI in personalization and memory.\nTextoshop: A novel tool that applies concepts from image editing to text manipulation. It allows users to engage in flexible and creative text editing by treating words and sentences similarly to how designers manipulate visual elements in software like Photoshop.\nNotable Contributions and Impact on HCI Kim\u0026rsquo;s work has significantly impacted how AI and human-centered design are integrated into real-world applications. By designing systems that bridge the gap between humans and data, he has contributed to making self-tracking technologies more accessible and adaptable to various user needs. His contributions to public health technology‚Äîespecially in using AI to enhance social and emotional well-being‚Äîhighlight the evolving role of HCI in addressing both technical and social challenges.\nSource \u0026ldquo;Understanding the Benefits and Challenges of Deploying Conversational AI Leveraging Large Language Models for Public Health Intervention\u0026rdquo; (2023)\nCareCall project paper\n\u0026ldquo;Textoshop: Interactions Inspired by Drawing Software to Facilitate Text Editing\u0026rdquo; (2024)\nTextoshop project paper\nAchievements Best Paper Award at CHI 2023 Best of CHI Honorable Mention in 2021 Recipient of the International Postdoc Fellowship from the National Research Foundation of Korea (2019) Conclusion Young-Ho Kim\u0026rsquo;s work at the forefront of HCI research continues to shape how we think about user interaction with data and technology. His contributions to flexible self-tracking systems and AI-driven health interventions exemplify the power of integrating advanced AI with human-centered design, making significant strides in both public health and everyday technology interactions.\n","date":"2024-10-03T00:00:00Z","image":"http://localhost:1313/images/researcher.png","permalink":"http://localhost:1313/post/hciresearcher/researcher/","title":"HCI Researcher"},{"content":"The Leap Motion Controller The Leap Motion Controller (introduced in 2013) is a gesture-based input device designed to allow users to control on-screen elements through hand and finger movements. Unlike traditional input devices like the mouse and keyboard, Leap Motion provided a touch-free, 3D interaction experience. It could track users\u0026rsquo; hand movements in real time, within an interaction space of about 8 cubic feet.\nGesture-Based User Interface Leap Motion falls under Gesture-Based User Interfaces (GBUI), which allow interaction through body movements. Specifically, it‚Äôs part of the broader Natural User Interface (NUI) category, focusing on natural and intuitive interactions.\nWhy Leap Motion Failed Despite its innovative concept, Leap Motion did not gain widespread adoption due to:\nLimited Use Cases: Its primary applications, like gaming and 3D modeling, appealed only to niche audiences. Most users found traditional input methods more practical for everyday tasks.\nTracking Issues: Many users reported inconsistent accuracy and latency, particularly when hands moved quickly or left the interaction zone. This made it hard to rely on for precise tasks, such as 3D design.\nLack of Developer Support: Limited developer interest meant fewer applications that leveraged Leap Motion‚Äôs capabilities, reducing user incentive to adopt the device.\nCompetition with VR/AR: As VR and AR technology gained popularity, Leap Motion struggled to stay relevant despite attempts to integrate with VR headsets.\nPotential for Future Success While Leap Motion didn‚Äôt succeed initially, the technology might find new applications as gesture-based interactions become more important in VR and AR environments. With its 2019 acquisition by Ultraleap, which combines gesture control with haptic feedback, this technology could offer more immersive experiences in fields like healthcare and remote collaboration.\nConclusion The Leap Motion Controller was a forward-thinking product that didn‚Äôt find immediate success due to limited use cases, technical challenges, and competition. However, with advances in machine learning and computer vision, its underlying technology may still shape future human-computer interactions.\n","date":"2024-10-03T00:00:00Z","image":"http://localhost:1313/images/leap.jpg","permalink":"http://localhost:1313/post/devices/inputdevices/","title":"Input Devices and Interaction Paradigm"},{"content":"Today, I‚Äôll introduce my first Unity 3D project, the classic Roll-a-Ball game.\nUnity Setup If you‚Äôre interested in how to set up Unity, check out my Set up Unity. I cover everything from downloading Unity Hub to configuring the environment for this project.\nBuilding the Basics Setting the Scene:\nI created a new 3D project in Unity and got started by adding a flat Plane as the ground. Resized it to (2, 1, 2) to give our rolling ball plenty of space to move. Adding the Player:\nNext, I created a Sphere object and named it Player. Moved the sphere up slightly by setting the Y coordinate to 0.5. This gave it a realistic \u0026ldquo;standing\u0026rdquo; position on the ground. Adding the Rigidbody Component:\nTo enable physics interactions, I added a Rigidbody component to the Player object. This step allows the ball to respond to forces and gravity. Creating a Player Controller Script:\nI added a script called PlayerController.cs to manage the ball\u0026rsquo;s movement. using UnityEngine; public class PlayerController : MonoBehaviour { public float speed = 10; private Rigidbody rb; void Start() { rb = GetComponent\u0026lt;Rigidbody\u0026gt;(); } void FixedUpdate() { float moveHorizontal = Input.GetAxis(\u0026#34;Horizontal\u0026#34;); float moveVertical = Input.GetAxis(\u0026#34;Vertical\u0026#34;); Vector3 movement = new Vector3(moveHorizontal, 0.0f, moveVertical); rb.AddForce(movement * speed); } } This script got the ball rolling!\nSetting Up the Camera For a more immersive experience, I wanted the camera to follow the player. Initially, I tried making the Main Camera a child of the Player object, but that led to a weird, shaky view. So instead, I wrote a CameraController script that allowed the camera to follow the ball smoothly, using a fixed offset.\npublic class CameraController : MonoBehaviour { public GameObject player; private Vector3 offset; void Start() { offset = transform.position - player.transform.position; } void LateUpdate() { transform.position = player.transform.position + offset; } } Adding the Pick-Ups with Score Variations I added two types of collectible cubes: PickUpGreenParents and PickUpOrangeParents. Each of them affects the score differently:\nGreen Pick-Ups: When the player collects these, the score increases by +1. Orange Pick-Ups: These are trickier! Collecting an orange pick-up decreases the score by -1. Creating Prefabs for Green and Orange Pick-Ups\nI created one green and one orange cube, then saved them as separate prefabs: PickUpGreenParents (colored green) and PickUpOrangeParents (colored orange). Prefabs made it easy to duplicate and spread these objects around the scene while keeping the setup consistent. Configuring Collider and Trigger\nEach prefab was tagged with PickUpGreenParents or PickUpOrangeParents so I could handle them differently in the script. I also added a Collider component to each with \u0026ldquo;Is Trigger\u0026rdquo; enabled. Updating the PlayerController Script\nTo manage scoring, I modified the PlayerController script to check each collected object‚Äôs tag. Based on whether the tag was PickUpGreenParents or PickUpOrangeParents, the script either increased or decreased the score. private void OnTriggerEnter(Collider other) { if (other.gameObject.CompareTag(\u0026#34;PickUpGreenParents\u0026#34;)) { other.gameObject.SetActive(false); count = count + 1; // Increases score by 1 for green SetCountText(); } else if (other.gameObject.CompareTag(\u0026#34;PickUpOrangeParents\u0026#34;)) { other.gameObject.SetActive(false); count = count - 1; // Decreases score by 1 for orange SetCountText(); } } Using the SetCountText Method I used the SetCountText() method to update the score display each time a pick-up was collected, so the player sees their updated score in real-time. void SetCountText() { countText.text = \u0026#34;Count: \u0026#34; + count.ToString(); if (count \u0026gt;= 2) { winTextObject.SetActive(true); } } The UI Text Hurdle: Adding the Score and Win Text Here\u0026rsquo;s where I had a small error. I wanted to display the score and a \u0026ldquo;You Win!\u0026rdquo; message, but Unity wasn‚Äôt showing the UI objects. I realized I had forgotten to make the CountText and WinTextObject fields in my script public, which prevented me from linking them in the Inspector. Without these fields being public, Unity can‚Äôt display the UI elements on screen, so I was left staring at a blank display.\nAfter making them public, I linked the CountText and WinTextObject objects in the Inspector. Here is the updat code !\npublic class PlayerController : MonoBehaviour { public float speed = 10f; public TextMeshProUGUI countText; public GameObject winTextObject; private int count; } Adding Accelerometer Input for Mobile I implemented accelerometer-based movement. This allowed the player to control the ball by tilting their device‚Äîa great way to enhance gameplay on smartphones!\nEnabling the Accelerometer\nTo use the accelerometer, I first checked if the device is a mobile device by using an isMobileBuild flag. I also used InputSystem.EnableDevice to activate the accelerometer input when running on mobile. Updating Movement Logic\nIn the FixedUpdate method, I checked for accelerometer data. If it was available, I used that data for movement; otherwise, I defaulted to keyboard input (for desktop testing). Here‚Äôs the updated FixedUpdate code: private void FixedUpdate() { Vector3 movement = Vector3.zero; if (isMobileBuild \u0026amp;\u0026amp; UnityEngine.InputSystem.Accelerometer.current != null) { // Get accelerometer data for movement Vector3 acceleration = UnityEngine.InputSystem.Accelerometer.current.acceleration.ReadValue(); AccText.text = \u0026#34;Accelerometer: \u0026#34; + acceleration.ToString(\u0026#34;F6\u0026#34;); // Display accelerometer data // Map accelerometer data to movement (adjust axes if needed based on phone orientation) movement = new Vector3(acceleration.x, 0.0f, acceleration.y); } else { // Use input from the non-mobile build (keyboard/controller input) movement = new Vector3(movementX, 0.0f, movementY); } // Apply movement force to the rigidbody rb.AddForce(movement * speed); } // Enable accelerometer if running on a mobile device if (isMobileBuild \u0026amp;\u0026amp; UnityEngine.InputSystem.Accelerometer.current != null) { InputSystem.EnableDevice(UnityEngine.InputSystem.Accelerometer.current); } Displaying Accelerometer Data\nTo help debug, I displayed the accelerometer data on the screen using a Text component (AccText). This allowed me to confirm the data being received and adjust the movement accordingly. Deploying as a Mobile Application After implementing accelerometer controls, I wanted to test the game on a mobile device. Since I‚Äôm using a MacBook and iPhone, I needed Xcode to build and deploy the app.\nXcode Requirement for iOS Testing\nInitially, I planned to use Unity Remote 5 for quick mobile testing. I went to File \u0026gt; Build Settings \u0026gt; iOS \u0026gt; Switch Platform to change the platform to iOS. I also opened Player Settings to configure my project for iOS. However, I discovered that without Xcode installed, Unity wouldn‚Äôt let me select iPhone under Editor \u0026gt; Device in Project Settings Installing Xcode\nTo resolve this, I downloaded and installed Xcode from the App Store. This process took a while, but it‚Äôs essential for iOS development on Unity. Using Unity Remote 5 for Quick Testing\nWith Xcode installed, I could finally set the platform to iOS and select iPhone under Editor \u0026gt; Device in Project Settings. I downloaded Unity Remote 5 on my iPhone, connected it, and could now use the remote app to test things like accelerometer movement without needing to build and deploy the app. Demo ","date":"2024-10-03T00:00:00Z","image":"http://localhost:1313/images/rollaballimage.png","permalink":"http://localhost:1313/post/lab3/unity3dapplication/","title":"My First Unity Project - Roll a ball"},{"content":"Reflections on Ivan Sutherland‚Äôs ‚ÄúThe Ultimate Display‚Äù In 1965, Ivan Sutherland wrote ‚ÄúThe Ultimate Display‚Äù, imagining a future where computers could create realistic, immersive worlds. His ideas basically set the stage for Virtual Reality (VR) and Augmented Reality (AR) as we know them today.\nWhat Sutherland Predicted and What‚Äôs Happening Now VR and AR: Sutherland envisioned virtual worlds that felt as real as the physical world. Today, VR headsets like Oculus Rift and AR tools like Microsoft HoloLens make that idea real by letting us experience and interact with digital content in immersive ways.\nTouch and Feel in VR: Sutherland also imagined being able to feel virtual objects, like sitting on a virtual chair that ‚Äúfeels‚Äù real. With haptic technology (gloves, vests, etc.), we can now feel textures, pressure, and vibrations in VR, bringing us closer to that vision.\nInteractive Graphics: He foresaw real-time interactive graphics, where users could control and manipulate digital objects. This idea led to the development of game engines like Unreal Engine and Unity, which power modern video games, training simulations, and 3D modeling.\nWhat Could Still Become Reality Full Sensory VR: Sutherland hinted at VR that stimulates all senses, including taste and smell. While current VR focuses on sight and sound, researchers are experimenting with smell and taste simulation. One day, VR could fully engage all our senses.\nBrain-Computer Interfaces (BCI): Although Sutherland didn‚Äôt directly mention it, his vision of seamless interaction hints at controlling VR with our minds. Projects like Neuralink are working on this, which could let us move in VR worlds using just our thoughts.\nShared Virtual Worlds (Metaverse): Sutherland imagined shared virtual experiences. Today, companies like Meta are building the metaverse‚Äîa persistent virtual world where people can meet, work, and socialize.\nConclusion Ivan Sutherland‚Äôs ideas in ‚ÄúThe Ultimate Display‚Äù were incredibly ahead of his time. Many of his predictions are now reality, and his vision still inspires new advancements in VR, AR, and immersive tech. His work is a reminder of where we started and where the future of human-computer interaction might take us.\nSource: The Ultimate Display by Ivan Sutherland\n","date":"2024-10-03T00:00:00Z","image":"http://localhost:1313/images/ivan.jpg","permalink":"http://localhost:1313/post/readivan/ultimatedisplay/","title":"Ultimate Display by Ivan Sutherland "},{"content":"Image Source: Anant Kadiyala Affordance gives users a visual hint on what actions they can take.\nFor example, when we see a button, we instinctively want to press it, or when we see a switch, we feel like pulling it. On an app or website, a rectangular box with a border makes us think we can click and type into it. These cues play into human psychology.\nThus, affordance is a crucial element of visual user interfaces. The clearer the visual cues are, the less ambiguity there is for the user to understand what action is expected.\nIn mobile environments, the importance of affordance becomes even more pronounced, as the small screen size and limited space make it harder for affordance to be as visually obvious. Therefore, clear affordance is critical to ensure smooth user interaction on mobile devices.\nGood case - Traffic lights that show the remaining time Image Source: Busan News\n1. Clear Information Display\nTraffic lights that show the remaining time provide users with clear information on how much time is left before the signal changes. Pedestrians can easily determine whether they have enough time to cross the street, and drivers can predict when the signal will switch. This intuitive display helps users make more informed and safer decisions. 2. Behavior Guidance\nThe countdown timer allows both pedestrians and drivers to act accordingly. Pedestrians can choose to cross quickly if there is enough time, or wait for the next signal if time is running out. This guides user behavior and enhances traffic safety by reducing risky actions. 3. Meeting User Expectations\nIn addition to simply showing red or green lights, the countdown timer allows users to predict signal changes more accurately. This design meets user expectations by providing precise information, reducing stress at intersections, and improving overall traffic flow. Bad case - Keyboard Image Source: https://www.clien.net/service/board/park/16744923\nThe placement of a power button above the delete key on a keyboard is a bad example of affordance.\n1. Risk of Accidental Use\nWhen the power button is located too close to the delete key, users are more likely to press it accidentally, potentially shutting down the system unintentionally. The functions are too distinct to be placed so closely. 2. Functional Mismatch\nThe delete key is frequently used, whereas the power button is not. Placing such an important function near a less frequently used key increases the risk of errors and reduces the overall usability. 3. Contrary to User Expectations\nUsers do not expect two very different functions to be positioned so closely together. This placement disrupts the intuitive understanding of how the keyboard should work. Solutions 1. Relocate the Power Button\nThe most straightforward solution is to move the power button to a location further away from frequently used keys like the delete key. For example, the power button could be placed at the right-top corner of the keyboard, near the function keys or in a separate, more isolated area where it‚Äôs less likely to be pressed by accident. 2. Add a Confirmation Step\nImplementing a confirmation step when the power button is pressed would prevent accidental shutdowns. For instance, instead of instantly shutting down the system, the button could trigger a prompt asking the user to confirm the action. ","date":"2024-09-19T00:00:00Z","image":"http://localhost:1313/images/affordance.jpg","permalink":"http://localhost:1313/post/affordances/affordances/","title":"Affordances"},{"content":"Image Source: By John O'Malley Dark patterns are UI/UX design techniques intentionally crafted to exploit human psychology and trick users into doing things they don‚Äôt necessarily want to do. The British UX designer Harry Brignull, who first coined the term, described dark patterns as follows\n‚ÄúA carefully crafted user interface designed to trick users into performing actions such as signing up for insurance or signing up for recurring bills.‚Äù\nIn other words, dark patterns refer to UI/UX that is cleverly and intentionally designed to elicit certain actions from users, whether they want to or not. These designs often serve business goals, such as increasing subscriptions or data collection, but they do so at the expense of the user‚Äôs autonomy, leading to unintended actions like accidental purchases or subscriptions.\nTypes of Dark Patterns Design Here are five types of dark patterns commonly seen\nNagging Image Source: ResearchGate\nNagging occurs when an app or website repeatedly interrupts the user‚Äôs progress with prompts or messages, draining their time and attention. These interruptions can make the user feel pressured or annoyed, leading them to eventually agree to the message or request‚Äîeven if it‚Äôs not what they want‚Äîjust to move forward.\nIf the interruptions happen frequently, the user might decide that giving in to the prompt is easier than continuing to dismiss it. This pattern is commonly seen in requests to subscribe to premium services, allow notifications, or share personal data, and it can result in a frustrating user experience.\nObstruction Image Source: Norwegian Consumer Council, 2018\nObstruction involves intentionally making certain tasks difficult or confusing for the user. It artificially complicates the steps required to perform actions that the user might want to avoid, such as canceling a service, deleting an account, or disabling ads. Designers create complex menu structures, lengthy procedures, and multiple confirmation steps to increase the likelihood that the user will give up.\nFor example, in this image, Facebook requires users to review and adjust their data sharing settings across multiple screens. Users must navigate through detailed information and various options to disable data sharing with advertisers. By making this process lengthy and complex, users are more likely to accept the default settings, even if they intended to limit data sharing.\nSneaking Image Source: Uxcel\nSneaking occurs when important information, such as additional fees or terms, is hidden from the user until the last possible moment. This pattern is often used to make a product or service appear cheaper or more attractive than it actually is, only revealing the true cost or consequences just before the user commits.\nFor example, an accomodation booking site may show a price at a discounted price, but hidden fees like cleaning costs is only added at the checkout stage. This tactic leaves the user feeling deceived and frustrated because they weren‚Äôt provided with full transparency from the beginning.\nInterface Interference Image Source: By Cai \u0026 Roberta\nInterface interference manipulates the design of user interface elements‚Äîsuch as buttons, links, or menus‚Äîto confuse or mislead the user. This often involves making certain options (like opting out or declining a service) difficult to see or access, or placing desired actions in locations where users are less likely to find them.\nIn this example, the user is trying to cancel a subscription, but the interface prominently displays a blue button that offers a discount (‚ÄúGive me US$1.00 off‚Äù), making it more noticeable than the ‚ÄúContinue to cancel‚Äù option. This visual manipulation influences the user‚Äôs decision, encouraging them to take the action that benefits the company (keeping the subscription) rather than proceeding with the cancellation.\nForced Action Image Source: blog\nForced Action refers to situations where the user is required to perform a specific action in order to continue using a service or complete a task. This pattern typically forces users to agree to terms, share personal information, or sign up for a service they may not want, simply to proceed with what they were doing. It can make users feel that they have no other option but to comply.\nFor instance, in this image, a user simply wants to shut down or restart their computer, but the only options available are ‚ÄúUpdate and shut down‚Äù or ‚ÄúUpdate and restart.‚Äù This design forces the user to perform an update, even if they would prefer to delay it, limiting their choice and making them feel compelled to take an action they may not want at that moment.\nExamples Instasize Premium Subscription Instasize is an app that helps people resize photos for Instagram, ensuring that they don‚Äôt get cropped when posted. Dark Pattern Explanation\nNagging\nWhile I‚Äôm editing photos, a premium subscription pop-up frequently appears, interrupting my workflow. Even though I‚Äôm not interested in upgrading, I keep seeing the same subscription prompt repeatedly, which creates a sense of fatigue and frustration due to the constant interruptions. Forced Action\nThe pop-up doesn‚Äôt have a close button, and I have to wait around 10 seconds for it to disappear before I can continue using the app. This lack of a ‚Äúclose‚Äù option makes me feel pressured into considering the premium subscription since I‚Äôm essentially held back from my task. How to Redesign It\nAdd an ‚ÄúX‚Äù or ‚ÄúClose‚Äù button to the pop-up so that users like me can dismiss it immediately if we‚Äôre not interested, giving us full control over our experience.\nInclude an option in the app settings that lets users turn off subscription prompts if they do not wish to see them.\nSkyscanner Price Discrepancy by Country Setting Skyscanner is a flight search engine that allows users to compare flight prices across different airlines and book their travel at the best price. I often use Skyscanner to find affordable flights for my travels. Price in Korea Setting\nPrice in France Setting\nDark Pattern Explanation\nSneaking\nWhen I set my country to Korea and search for a flight from Paris to Korea, the price displayed differs from the price shown when I set the country to France. This pricing difference is not clearly communicated to users, and the cheaper option is hidden unless they change their location setting. This can cause users to unknowingly pay more for the same ticket. Interface Interference\nThe interface automatically adjusts the price based on the selected country, limiting my ability to see or choose the best price across locations. Because there is no transparent way to compare prices across countries, users like me may miss out on the lower price option. How to Redesign It\nAllow users to easily see the price differences between countries by adding a \u0026ldquo;Compare prices across countries\u0026rdquo; option. This would help users like me make informed choices by seeing all available prices.\nDisplay a note explaining that prices may vary depending on the country setting, giving users full transparency about potential savings if they adjust their location.\n","date":"2024-09-19T00:00:00Z","image":"http://localhost:1313/images/darkpatterndesign.jpg","permalink":"http://localhost:1313/post/dark-design-patterns/darkdesignpattern/","title":"Dark Design Patterns"},{"content":"Image Source: Gestalt Principles\nGestalt is a German word meaning \u0026lsquo;form\u0026rsquo; or \u0026lsquo;shape.\nIt refers to how people perceive visual elements in a given situation. Generally, we compare visual patterns and past experiences to make sense of what we see. We often perceive these elements as a single whole, rather than as separate parts. By connecting the elements, recognizing familiar shapes, sharing information, and filling in the gaps, we make sense of the overall picture.\n1. Law of Proximity Image Source: By Cameron Chapman\nThe Law of Proximity descrives the phenomenon in which element that are close to each other are perceived and felt as a group.\nFor example, in the left image, the dots don\u0026rsquo;t appear to be grouped. However, in the right image, the dots are closer together making them appear as three distinct groups. This proximity can help in organizing related content by placing them close to each other.\n2. Law of Similarity Image Source: By Cameron Chapman\nThe Law of Similarity means that objects with similar shpes, sizes, colors, or other attributes are perceived as part of the same group.\nIn this example, all shapes are squares, but the difference in color causes our brain to group the green squares together and the gray squares together, even though they share the same shape. This shows how color similarity plays a key role in organizing visual elements into groups.\n3. Law of Closure Image Source: By Cameron Chapman\nThe Law of Closure refers to the tendency to perceive incomplete shapes as whole or complete. Our brain fills in the missing parts, allowing us to recognize an entire form even when elements are missing.\nIn the image, some parts of the shapes are missing, but our brain automatically fills in the gaps, making us perceive the incomplete shapes as a complete form. This principle is commonly seen in logos or icons where parts of the design are missing, yet we still recognize the full shape.\n4. Law of Figure-Ground Image Source: By Cameron Chapman\nThe Law of Figure-Ground describes how we distinguish an object (the figure) from its surrounding background (the ground). Our focus shifts between the object and the background, depending on what we are focusing on at any given moment.\nIn this image, depending on where we focus, we may see either the foreground shapes as the figure or the background. This principle is often used in visual illusions, where the brain toggles between seeing two different images depending on whether it focuses on the figure or the ground.\n5. Law of Continuity Image Source: By Cameron Chapman\nThe Law of Continuity states that elements arranged on a line or curve are perceived as related or continuous. This principle explains how our eyes follow the smoothest path when interpreting visual elements.\nIn this image, the red and gray dots form continuous curves. Even though they are separate dots, our brain perceives the curves as a single continuous path, demonstrating how continuity helps us organize visual elements in a flowing pattern.\n6. Law of Common-region Image Source: Uxcel\nThe Law of Common-region states that elements located within the same boundary are perceived as part of a group. A visual boundary such as a box or a color background can create this perception of grouping, even if the elements are not physically close.\nIn this image, the circles inside the box are perceived as a group because they share a common region (the box). Even though the circles outside the box are the same size, shape, and color, they are seen as separate because they are not within the same boundary.\nApplications of Gestalt laws in daily life 1. Confusing stairs Image Source: Bright Side\nProblem Law of Continuity\nThis pattern has consistent stripes, which causes our eyes to fail in distinguishing where the stairs end and begin, making the surface appear like a continuous plane. In situations where step differences need to be recognized, this pattern can create visual confusion, which poses a safety risk. Law of Figure-Ground\nThere is a lack of clear distinction between the figure (the stairs) and the background. As a result, it becomes harder to perceive the shape and depth of the stairs, and users may struggle to identify the height changes of the steps. Solution Clear Boundary Markings\nAdding a different colored stripe to the edges of each step can help clearly distinguish the boundary of the stairs. This will break the continuity and allow the steps to be seen as distinct units. Enhancing Contrast\nTo better separate the background and foreground, stronger color contrast can be applied to the steps. For example, using a brighter color on the edges of the stairs will help clearly define the steps, making it easier to perceive the changes in height. 2. Confusing Buttons on YES24 Image Source: YES24\n*YES24 is a popular online bookstore in South Korea, where users can purchase physical books, eBooks, and other products.* Problem Law of Similarity\nThe NPay Purchase and One-Click Purchase buttons look similar in color, which can confuse users. Since both buttons serve different functions but appear visually similar, users may accidentally choose the wrong payment option. Because Naver\u0026rsquo;s branding is strongly associated with green, the green color of the NPay button helps signify that it is a Naver-related payment method. However, the other buttons are also visually similar, which may lead to mistaken selections. Law of Proximity\nThe Buy, Add to Cart, NPay, and One-Click Purchase buttons are placed very close to each other, which can make it difficult for users to quickly differentiate between actions. When buttons with different functions are positioned without sufficient spacing, they are perceived as part of a single group, leading to potential confusion and misclicks. Solution Color Differentiation\nKeep the NPay button green to signify its association with Naver, and use a different color for other purchase options, such as One-Click Purchase. This will allow users to easily distinguish between payment methods based on color, reducing the chances of accidental selections. Spacing Adjustments\nIncrease the spacing between the Buy, Add to Cart, NPay, and One-Click Purchase buttons to create a clearer separation of actions. By adding more space, each button will stand out as a distinct option, making it easier for users to select the correct action without confusion. ","date":"2024-09-19T00:00:00Z","image":"http://localhost:1313/images/gestalt.jpg","permalink":"http://localhost:1313/post/gestalt-laws/gestaltlaw/","title":"Gestalt Law"},{"content":"Today, I\u0026rsquo;ll show you how to set up a blog using Hugo and deploy it on GitHub Pages. I\u0026rsquo;ll be working on a MacBook üíª (Apple silicon).\nWhy Hugo? No server-side code: Hugo builds purely static files, so there is no need to manage any backend infrastructure. Fast to render: Static sites are quick to render because they are pre-built into HTML before deployment. No dynamic content: While this means no interactive elements like forms or real-time data updates, it also simplifies maintenance. No database: Content is stored as files, not in a database, which reduces complexity and overhead. Often more secure: Fewer security vulnerabilities as there are no databases or server-side scripts to attack. No real-time UI: Hugo sites don‚Äôt support real-time updates or interactions without additional tools. Content is versioned: With git, all content can be version controlled, providing an easy way to manage changes and rollbacks. Prerequisites Before starting, make sure you have Homebrew and Git installed on your MacBook. If you don\u0026rsquo;t have it, you can install it with the command below:\n/bin/bash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\u0026#34; To check if Git is already installed, run the following command:\ngit --version If Git is not installed, you can install it using Homebrew:\nbrew install git Once installed, verify it by running git --version again to ensure everything is set up correctly.\nStep 1: Install Hugo To install Hugo, first you open the terminal and run this code\nbrew install hugo Once the installation is complete, verify the installation by checking the Hugo version\nhugo version #and if installed correctly, you should see an output similar to the following Hugo Static Site Generator v0.74.3/extended darwin/amd64 BuildDate: unknown Step 2: Create a New Hugo Site Now that Hugo is installed, you can create your new blog. Navigate to the directory where you\u0026rsquo;d like to store your blog and create a new site with the command below:\nhugo new site \u0026lt;nameOftheSite\u0026gt; Replace nameOftheSite with your desired name for the blog site.\nStep 3: Add a Theme To give your blog a proper design, you\u0026rsquo;ll need to add a theme. Hugo has a variety of themes available that you can browse and choose from.\nGo to the Hugo Themes website to browse available themes. Once you find a theme you like, click on it. On the theme\u0026rsquo;s page, there will be a \u0026ldquo;Download\u0026rdquo; button. When you click this button, you\u0026rsquo;ll be redirected to the theme\u0026rsquo;s GitHub repository. You should check the ReadMe file on the GitHub repository for instructions on how to apply the theme. For example, I used the hugo-theme-stack theme.\nFirst, initialize git in your project directory (ensure you are in the correct directory):\ngit init Then, add the theme as a git submodule:\ngit submodule add https://github.com/CaiJimmy/hugo-theme-stack/ themes/hugo-theme-stack Now, open the hugo.toml file and configure Hugo to use this theme by adding the following line:\ntheme = \u0026#34;hugo-theme-stack\u0026#34; Each theme has different configuration options, so make sure to follow the specific instructions in the theme\u0026rsquo;s GitHub repository. You can also customize it further based on your needs.\nStep 3.1: Creating New Posts To create a new blog post, you can use the following Hugo command:\nhugo new posts/\u0026lt;post-name\u0026gt;.md This will create a new Markdown file in the content/posts/ directory. You can then edit this file to write your post.\nFor example:\nhugo new posts/my-first-post.md Step 4: Create a New GitHub Repository To deploy the site to GitHub Pages, you\u0026rsquo;ll need to connect it to a GitHub repository.\nFirst, create a new repository on your GitHub account.\nIn my case, I created my repository with the name \u0026lt;username\u0026gt;.github.io and left it public.\nOnce the repository is created, you\u0026rsquo;ll see a setup page. Here, you\u0026rsquo;ll find instructions on how to push your existing files to this repository.\nGo back to your terminal and connect your Hugo project to this GitHub repository by following the commands on the setup page (red box).\nStep 5: Deploy to GitHub Pages We\u0026rsquo;ll use GitHub Actions to automatically build and deploy the Hugo site to GitHub Pages whenever changes are pushed to the main branch.\nI followed the steps indicated in the official Hugo documentation: Host on GitHub Pages.\nBefore you deploy it, you should change the baseURL in the hugo.toml file to your GitHub Pages URL.\nFor example,\nbaseURL = \u0026#34;https://jiwonyziyo.github.io/\u0026#34; Step 6: Access Your Site Once the GitHub Action finishes running, your blog will be deployed to GitHub Pages. You can check your blog by visiting the URL found in Settings \u0026gt; Pages.\nTo see the deployment status, you can also go to the Actions tab in your GitHub repository. If everything went well, you\u0026rsquo;ll see that your site was successfully deployed.\nVoil√† ! Now you have a fully functioning personal blog! ","date":"2024-09-19T00:00:00Z","image":"http://localhost:1313/images/hugo.jpg","permalink":"http://localhost:1313/post/lab1/setupblog/","title":"Set up Blog"},{"content":"Today, I\u0026rsquo;ll show you the process of setting up Unity on a MacBook üíª.\nStep 1: Sign Up for a Unity Account To start using Unity, you\u0026rsquo;ll need to create an account. You can sign up on the Unity website.\nOnce your account is created, you will have access to Unity Hub.\nStep 2: Verify Student Status for Unity Education License Unity offers free premium access to students through the Unity Student Plan. Follow these steps to verify your student status:\nGo to the Unity Student Plan page.\nClick on Free Access: Post-Secondary.\nLog in with the Unity account you created earlier. (I used my personal Google account for registration but verified my student status using my school email.) During the verification process, you will need to provide your school information and expected graduation year.\nAfter verification, you will receive a completion email. Click on Redeem my Student plan to finalize the process. Make sure to log in using the same email you used to sign up, not your school email.\nYou will receive a license key via email, which you will use during the Unity installation process.\nStep 3: Install Unity Hub Unity Hub is a central application for managing Unity installations, projects, and licenses. Here\u0026rsquo;s how to install Unity Hub on your MacBook:\nVisit the Unity Hub download page.\nDownload and install Unity Hub for macOS.\nOnce installed, open Unity Hub and log in with your Unity account.\nNow you need to add a license. Click on Preferences in Unity Hub.\nIn the menu on the left, go to Licenses and click Add. Select Activate with serial number, then enter the license key you received via email.\nYour license, which grants one year of access to Unity, will now be active.\nStep 4: Install a Specific Version of Unity For my HCI classes, I need to use a specific version of Unity rather than the latest one. Here‚Äôs how you can install a different version:\nOpen Unity Hub.\nGo to the Installs tab.\nClick on Install Editor to install a new Unity Editor version.\nIn the Choose a version section, select Archive from the dropdown to see older versions of Unity.\nSelect the required version for your class or project.\nChoose the platform modules you need (such as iOS, Android, WebGL, etc.).\nClick Next, and Unity will begin installing the selected version.\nStep 5: Set Up Your First Unity Project Once Unity is installed, you can create your first project:\nOpen Unity Hub and go to the Projects tab. Click New Project.\nSelect a template (e.g., 2D, 3D, etc.) based on the type of project you want to build. Name your project and choose a location for it.\nClick Create.\nUnity will set up the project, and you\u0026rsquo;ll be taken to the Unity Editor where you can start developing.\nVoil√† ! Good luck, and enjoy building with Unity!\n","date":"2024-09-19T00:00:00Z","image":"http://localhost:1313/images/unity.jpg","permalink":"http://localhost:1313/post/lab2/setupunity/","title":"Set up Unity"}]